# Donald Hoffman - Interface Theory of Perception (ITP) Notes

Purpose & Dates:

- March 25th (Friday, 11:59pm) - video presentation
    - This video presentation is a summary AND critique of Donald Hoffman’s 44-page paper on the Interface Theory of perception (≤ 15 mins).
    - Should have 1 - 3 criticisms, but the more the better.
- April 1st (Friday, 11:59pm) - Debate proposition
- April 8th (Friday, 11:59pm) - Debate rebuttal
    - everyone is going to take a position FOR the ITP, and one AGAINST the ITP. Everyone posts/discusses points and ideas from the paper that support, or are flawed in order to make their point.
    - each post (if in post/written format) is ≤ 750 words.

- The main points and ideas of the ITP are generally complex and very sophisticated. Thus, there is some supplemental material:
    - [Ted Talk](https://www.ted.com/talks/donald_hoffman_do_we_see_reality_as_it_is?share=1dfeab9462)
    - [Article](https://www.quantamagazine.org/the-evolutionary-argument-against-reality-20160421/)
    - and a revised version of the ITP.
- The target paper is the 44-page paper of the ITP.

# Defining the ITP & Reasoning behind it

- biological functions are most often affected by evolution and natural selection to favor functions that are the *most* beneficial to survival, and what the organism is meant to do
- however, in terms of *perceptive functions*, this theory proposes that natural selection favours non-truthful interpretations of reality, which inevitably help us survive
    - “non-truthful” refers to perceptions that hide object reality through symbolic representations
- it is pretty contradictory then, that vision is so accurate. specifically, we can reason that vision is really only useful *because* it is accurate and represents the world as is, instead of being an interpretation of it. Logically, through thousands of iterations of evolution, those that could perceive visually more accurately had an edge over those that had less visual clarity.
    - this is reasoned thru Knill et al, Marr, Geisler and Diehl, and Trivers.
    - However, the ITP aims to dispel this reasoning about how accurate and true the perceptions of vision, smell, and hearing are.
- this falseness can be seen through Monte Carlo simulations of evolutionary games.
    - in which those that accurate estimate reality do not outcompete the perceptions of any equal complexity, which do not estimate reality but are tuned to the relevant fitness functions
        - a Monte Carlo simulation is like running a game a million times, and using random sampling to create a population and sample to derive probabilistic outcomes from
            - think about flipping a coin a million times, and calculating the outcomes of all combinations of outcomes.
- the **fitness function** is entirely dependent on an organism’s *state* and *action,* combined with the state of the environment (objective reality)
    - and these functions beat out objective reality in terms of evolutionary competition
        - as if it is more beneficial for survival of an organism, to interpret reality in a more suited way, based on the organism’s function
        - think of how beneficial it would be for an amoeba to have a human’s vision, it wouldn’t benefit them because they have an entirely different fitness function
    - this is solidified by those Monte Carlo simulations of probability. Organisms that accurately estimate reality will never beat organisms that are more tuned to perceiving reality in benefit to their fitness function
        - think of the number of infinite possibilities, the classes of many different combinations of organisms that interpret and objectify reality. The class of organisms that accurately estimate reality are a very small subclass of organisms that interpret reality. And there are many, many more organisms that interpret reality to benefit their fitness function in varying degrees.
        - This is the **fitness-beats-truth** theorem

- Models of perception agree that the fundamental physics of the world are the *most appropriate* to describe reality, such as space, time, and shape. However, the ITP disagrees, because this language is inherently wrong to describe objective reality - because they are within the lens of our own creation, which by the ITP, is not an objective estimate of reality.

# Informal Introduction

- The ITP, informally, is a way to describe how reality exists *to us* in terms of how we perceive it.
- The ITP is analogous to the interface of a computer
    - when we interact with computers, we see icons and images of files and buttons, and characters.
    - however, those objects in the computer, are not actually stored as images and pieces of paper with text on them
        - essentially, the interface that we use to interact with the computer, is a mismatch to how computers actually operate
        - if we also think about the *language* of the interface, the language is a representation of how computers operate, that is totally different from how they work
            - files don’t have colors, colors are not actually there, they are generated, etc.
            - computers are digital, meaning that they represent all the information through electricity, thru *binary* (1’s and 0’s). text files and the characters within them are not stored as they appear, but rather entirely as arrays of values. those values have meanings, which are interpreted by the interface, to display the characters.
    - thus, the purpose of the interface is to obscure the complexity of the computer.
        - you can imagine, that if you had to program in the lowest-levels of language, that using a computer would NOT be something that would be popular.
        - if you want to get even more granular, try thinking about how difficult it would be to have a battery, and wires to each terminal, and to represent a 1, you had to tap the wires together. think about how much effort it takes to do that a couple of times, and try sending a digital email by tapping out manual 0’s and 1’s.
            - modern processors have speeds of 4 gigahertz, which is about 4 billion cycles *per second*. processors do the simplest computations of adding, storing values to make thing happen.
    - for the computer, the interface and how easy it is to use, defines the fitness function for its “survival” (usage)
        - thus, for the survival and usage of the computer, it’s important for it to be *usable* and not complex.
        - “perception is not about seeing the truth, it’s about having kids”...
            - in this way, the “space” and “time” of the real-world translates to the “buttons” and “folders” of the computer, that make the reality of the interface itself, but not the object reality that it attempts to hide.

- The importance of ITP is that it essentially is a contradiction to “space-time” itself, and that the truth of ITP disconfirms what we know of theoretical physics as a language to describe the objective world.
    - this theory predicts then that no physical object has any dynamic physical property (property, spin, momentum) when it is **not** observed.
        - but is it possible to observe that physical objects have no physical properties, when they are not observed?? this is seemingly contradictory, but this experiment has been done before and has been repeated, and its result is that predicted by ITP.
            - Ansmann et al, Cabello et al, Fuchs, Guistina et al, Pan et al, Rowe et al, Salart et al, Weihs et al, Mermin.
    - this theory and its prediction flips everything that we know - that the causality between the objects we observe is not true to the object reality as they truly are.
        - the cause is something that we observe and rationalize to understand, but is not an actual objective observation, because we do not observe objectively, and this rationalization serves to guide adaptive behaviour.
            - again, the interface of the computer. the interaction between two objects may be an apparent cause to the interaction, but are in actuality - not how they really work.
                - for instance, “deleting” a file or pressing an icon-button does not truly do those things in the computer, but is an interpretation.
    

- thus, causal representations of the objects we see are then wrong. in neuroscience, it can then be reasoned that the causality of neural activity on consciousness is true then, right?
    - not necessarily, because this causality may be the “interface” of how we adaptively interpret what we know about the brain, but according to the ITP, their relationship of causality is not true, and then we cannot say that neural activity causes any behaviour or conscious experience.
    - however, this does NOT mean that we should not take the ‘interface’ interpretations as NOT serious. Just as approaching a venomous snake is to deleting a file, you wouldn’t do it carelessly
        - that’s because it may not be “causal” that being careless with snakes, cliffs, etc. is tied with death, but it really is about the fact that our ‘interface’ or ‘interpretation’ of the surrounding world is meant to benefit survival and ‘fitness’, thus in our view, snakes and cliffs are things we should stray away from
    - also, the consensus about the objects we see as a species does NOT mean we can conclude about objective reality - because we are all the same species and thus have the same interpretation of reality, so we each agree on the interpretation.
        - think of the “Necker Cube” on the right, this is a *depiction* of a 3D object. It is our own interpretation of what a cube is, represented in 2D space. This is simply an ‘icon’ of interpretation, as it is flawed and limited, simply by its own ambiguity; it represents either a 3D cube in 2D space, or an impossible cube depending on your interpretation.

- What about the ability for us to use *science* to achieve remarkable accuracy and achievements? This is answered by the invention of symmetry theorem **(IOS)**
    - which basically boils down to the fact that we have constructed an interpretation of 3D space and 1D time, and can perfectly coordinate *within that perceived space-time* because it is our own (arbitrary) interpretation of those things.
        - the “bounding the cardinality on the set of states” is simply the idea that our interpretations are meant to put the reality that we observe into a finite amount of states, the combination between space x time has infinite states, which is not something that is even knowledgeable or understandable.
    - essentially, the FBT and the IOS come together to show that everything we have thought about, and theorized as humans has come from a fitness-benefiting interpretation process that shapes our ‘interface’ to aid survival, and thus any of our ideas fall under the IOS theorem, which means that our knowledge and actions are ‘self-fulfilling’ to act within the realm of interpretation that we have created.

- The Invention of Symmetry theorem is better explained thru **shape perception**, which is founded by Pizlo.
    - This essentially states that we can perceive three important things about shapes: symmetry, planarity, and compactness
        - symmetry of a shape itself
        - planarity of the shapes contours + edges
            - relating to planar graphs, 3D objects abstracted into planar graphs
                - the Necker cube is *not planar*
        - compactness of the maximal volume given the surface area.
            - basically, the things we perceive are known to be constructed as 2D images in our retina. However, we are supposedly perceiving 3D objects in the 3D space that we live in. However, this is a disconnect because given a 2D image, we perceive a 3D object. However, we cannot use the constraints of symmetry to reconstruct a 3D object from a 2D perception

![Untitled](Donald%20Hoffman%20-%20Interface%20Theory%20of%20Perception%20(I%2053e3e209c8064ddb9f97de7a4bb071d4/Untitled.png)

- CRITICISM?
    - perhaps it might be too abstract for me to fully understand or know if this section about shape perception has come up with a workaround, but as humans, we use 2 eyes to perceive the space, which gives us different cues about objects. So, in terms of strict image-vision, we may only receive a 2D image, we are really perceiving a 3D space because we have additional information about objects that allow us to understand depth (the 3rd axis).

- CRITICISM
    - Understandably, this paper is quite complex, abstract, and high-level in a way that is totally unlike most psychology, which is based on observable metrics. Normally, in theories and papers we are expected to concretely define the constructs and ideas that we use to be able to be concretely objective. This theory has a hard time ‘dumbing it down’ for many different concepts, but the most interesting are of “symmetry, planarity, and compactness”.
        - even though I have taken courses and learned about graphs, and heavy theoretical maths, these concepts are new and totally unexplained for the reader. Without any context, these measures of visual perception make no sense, and are better explained in the Pizlo paper, even though this paper tries to link shape perception to the IOS theorem.
        - this reference really only serves to link ITP to some proven ideas on shape perception, but makes no further link to show *how* it is related in any other form other than a very basic one.
    
- The fact that Darwinism has been founded on the objective reality means that it assumes that space-time, objects, organisms, DNA, etc, are objective and truly real. However, “universal Darwinism” has been understood to be an **algorithm** of variation, selection and retention. So, this abstraction and realization of this algorithm makes it able to be applied to domains that are not biological, such as internet memes, and thus does not really need the assumption of objective reality.
- Think of Schrodinger’s Cat and quantum mechanics. According to ITP, things don’t exist when we are not observing them, because to us, we do not concretely know if they are still there, until we observe them.
    - There are things that exist when we do not observe them, but these things are not part of our ‘realm’ of space-time. Any construct we have about objects, such as DNA, is only a representation of the concept, and not the actual object.
- Proximate questions are about how biological systems operate, through what mechanisms they work. Ultimate questions are about *why* they do what they do, and are more about why evolution has guided that system to operate in the way that it does, instead of any other way (ie. what benefit did this mechanism have over another?)
    - Proximate questions need proximate answers that describe the mechanisms and algorithms, whereas Ultimate questions need a more ultimate explanation that uses evolutionary causes
    
- Illusions are due to ways that a “proximate” system can fail or be tricked, causing non-truthful observations about objects to occur. Typically, they come from *heuristic* processes in the visual system, meaning that these are systems that have adapted to predict very well the stimuli and have efficient and accurate processing. However, these ‘shortcuts’ the visual system takes can lead to false assumptions that cause illusions. Thus, illusions are based on the fact we do see objectively, and only occur when assumptions are wrong and do not match reality.
    - However, the ITP is based on the fact that we DO NOT perceive objectively, thus, illusions cannot exist. The ITP instead defines illusions as unadaptive perceptions.

# Formal Explanation (oh no)

- ITP is a **perceptual strategy,** which maps objective reality to perceptual experiences. In fact, it is one of all possible perceptual strategies that are possible. This is done through mathematical classification of the components of perceptual strategies, and objective reality.
- We define the sets that we care about:
    - O is the organism
    - $W^T$ is the **total reality** which the organism is a part of
    - $W$ is the *objective reality* that the organism is NOT a part of ($W = W^T - O$)
    - X is the set of the organism’s *perceptual experiences*, where each member of X is a specific perceptual experience of the organism.
        - Thus, theories of perception will assign structure and function to these sets

- Generally, there is a relationship between the objective reality and an organism’s experiences, represented by the map *P*, which is defined as $P : W \rarr X$.
    - which means that the map P has no inherent structure, and it allows the case that X is a proper subset of W
        - $P : W \times X \rarr X$
    - thus, a **perceptual strategy** is the map $P : W \rarr X$.
        - meaning that P related perceptual events to events in objective reality, because these perceptual outcomes must inform the organism of real events.

- So, we must have perceptual events with properties, such as the *complement*. If there is a perceptual event such as “tastes like chocolate”, then there must be a complementary event, such as “does NOT taste like chocolate”.
- Perceptual events must also have *union* and *intersection*, so if there are two different perceptual events, they must also exist in the union of each other and their intersection
    - union - “x or y”
        - either exist
    - intersection - “x and y”
        - both exist
- Perceptual events thus are not single points, but sets. So, perceptual events are *subsets* of X. The collection of ALL events is denoted by a curly X, which is closed under complement
    - meaning that for any event in the set of all events, its complement will also be in that set of events.
    - also closed for union, and intersection.
        - if collections of events are *countable* and there is closure under a *countable union*, then this structure is called a $\sigma$-algebra.

- the mapping P then respects the $\sigma$-algebra structure on events $w$ of $W$, and $x$ on $X$. for every event E in $X$, this E is “pulled back” by P to a counter event $P^{-1}(E)$ which is in W.
    - a “measurable” function.
    
- Thus, an **interface** perceptual strategy is a measurable mapping $P: W \rarr X$
    - interestingly, this means that if X is within a 3D space with Euclidean constraints, that does not mean that W is 3D and Euclidean. Which means that perceptual strategies are not necessarily mappings from 3D to 3D.
        - in linear algebra, we can transform from 3D to 2D, but not 2D to 3D.
        
- perceptual strategies are called critical realist when the interface strategy is a **homomorphism** of all structures on W.
    - a homomorphism is a structure-preserving map.
    - critical realist strategies are proper subsets of the interface strategies

- interface strategies that are NOT critical realist are called **strict interface** strategies, which are non-veridical.

- Fitness-beats-Truth Theorem applied with the different types of perceptual strategies
    - using a game environment with competing organisms using different strategies, with resources being randomized and their payoff functions too, we can see which organism thrives most, and add complexity using territories.
    - strategies are dependent on the proportion of individuals that use the strategy
        - think about hunter-gatherers, where some hunt and gather, and others just free-load. If more populous works and gathers, then the free-loaders thrive, but if there are more free-loaders, then the “fitness” of the strategy declines until everyone free-loads and thus everyone starves.
    - within evolutionary game theory, organisms with fixed strategies from an infinite population are chosen completely randomly. the payoff function equates to fitness, such that more fitness means more reproductive success, and thus better competing strategies reproduce more quicky and outcompete.
        - Thus, we can use a replicator function to define the metrics of competition, by mapping all the payoffs of each possible competition between  every strategy, and the frequency of strategies in populations.
    - Monte Carlo simulations clearly show that veridical strategies go extinct when competing against strict interference perceptions of *equal complexity,* and also in many cases when the perceptual states is less than the veridical strat.
        - this is because for these strategies, truth is irrelevant, because only fitness is relevant to the outcome of competition. Thus, they waste no resources estimating reality by using only resources for actions that benefit their fitness function
    - It’s well assumed that as the world becomes more complex, it would be more beneficial to have a veridical perception. However, the FBT shows that this is backwards, it’s more useful to have non-veridical perceptions of the complex world, as the organisms with veridical perceptions becomes extinct. Furthermore, more complex worlds do not make veridical perceptions more fit.
    - Pinker suggests that humans are flawed in many ways other than just perception, especially among cognition. truth is not desirable, because it is clearly in the facts of the history of human beliefs. This is fundamental to FBT, as natural selection diverges us from the truth to make us more fit.

- Perception is not a passive process, it is very active. We are perceptual agents that receive information from the environment and process it, and develop knowledge, expectation, and consequence in order to understand actions.
- Alan Turing and his machines are simple forms of machines with an alphabet, a set of states, starting state, and a set of stopping states, and rules that determine what symbols from the alphabet, in each state, lead to other states.
    - The Church-Turing thesis says that all algorithms and procedures can be modeled in a Turing Machine.

- In lieu of Turing Machines and Finite Automata, we can define a perceptual agent to be machine that has:
    - current perceptions (X)
    - a set of actions (G)
    - a mapping of all possible combinations of reality, perceptions with the possible algebraic combinations of perceptions (not, and, or) to a range of values [0,1], which are probabilities.
    - a decision map that maps perceptions, actions, and all combinations of actions to probabilities
    - an action map, that maps actions to reality, and all possible combinations of reality to probabilities
    - and a discrete integer counter which adds 1 to the counter every time new perceptions happen

- Bayesian decision theory is a simple theory that places cost vs. benefit to decisions and their consequences. The module of a *perceptual agent* follows Bayes theory, because any agent that does not makes the agent incoherent in its actions and defeats itself (ie. it does not make optimal choices).
    - The main difference between BDT and the PAT model is that BDT attempts to estimate the *true state of the world* whereas PAT uses Markovian Kernels to model *perceptual interfaces* that are tuned to fitness functions, and not the world.

- ITP shows a new interpretation of previous work on psychphysics, like how we perceive “objective” events such as tone or weight, etc.
    - Because ITP believes that we cannot observe reality as it truly were/is/will be, then thus those objective events are never objective because we must observe them
        - “if a tree falls and there’s no one to hear it, did it really exist?”
    - thus, the perceived loudness of a tone is entirely dependent on our “interface” because without us, or the tone to create that interaction, there is no reality in between.
        - so this is an observer-dependent feature, which is everything that we observe, so we cannot truly discern reality from what we observe.
        - even with technology, which is seemingly unbiased to our own perception of objects and space-time, those technologies are observed by us
    - this does not null those psychophysical laws, but rather shifts their perspective from reality to interface. they dictate the relationships between aspects of our interface, rather than reality and ourselves.

- Perceptual Agents can be connected into networks by connecting action maps from one, to perception maps of another. A network can also be considered a ‘single’ agent, which makes it modular and object-oriented, and a away to model many problems like motion-correspondence, structure-from-motion, and combined into one machine that does both.

# Discussion

- Normal theory posits more-or-less, that we only see the reality that is necessary depending on our fitness function, otherwise, we use interfaces and interpretations of reality as heuristics to reality.
- However, ITP is a radical version of that, which is based on the heavy theoretical findings of evolutionary game theory, that it’s none of our perceptions are truly reality, and any of the constructs that we came up with as humans (space, time, objects, colors, motions, tastes, smells, etc.) are not real, but rather based on our interpretations of reality - ways that we interface with reality.
- Consciousness as a problem is too hard to solve, because it is so far removed and able to be imagined by the way we have constructed reality as we *think* we see it
    - in computation and math, there are classes of problems. P, NP classes of problems. Some problems are just inherently unsolvable, meaning that we cannot find an algorithm that will always give us an answer. We can, find answers, but not a set of steps to *always* find an answer.
    - consciousness cannot be stated through neural activity because there is no inherent causality between neurons and conscious experience, according to ITP.
        - this is because all our languages to define reality as we see it, evolved from a need to survive. Thus, we are limited by our constructs and cannot explain consciousness because it was not required for us to evolve language to do so.
- Overall, the concept of ITP is a much broader and abstract way to absolve the causality that we understand from us and reality at all, because we are the ones who’ve created our own realities, our own interfaces. This is not the newest idea, as Kant, Hawking and Mlodinow have proposed ideas similar that put the observer farther away from reality, because every single experience we have as individuals, is immediately biased by our own brain.
    - even then, ITP further removes us from even understanding ourselves as causing our own perceptions, because of the lack of reality and causality, who can really say that their own brain causes their own consciousness?

# Script for Summary & Critique

## Introduction

Starting off, this paper is so extremely mind-blowing. It is a huge mind-fuck that stopped me in my tracks. This paper makes you want to have a coffee and a cigarette in the rain. This paper is my perfect agent of solipsism, existentialism, and computation. Not only was it interesting, it was right up my alley with the amount of complexity it had. Not once yet have I read a paper this insane, that it combines some of the most difficult aspects of my program, like linear algebra, graph theory, and abstract theories of computation. Personally, it makes me look forward into thinking about more opportunities for peppering in research of my favorite parts of computer science, theory and math, with the construction of psychological phenomena. 

This paper is a very difficult and abstract read, and uses a lot of computer and math theory to help. A big part of computer science and math theory is introducing theories using informal theories to be able to get good grasp on the concept, followed by a formal definition that solidifies the theory using a rigid language. This paper does this very well, because it continuously interweaves good explanations in detail. So, we’ll start with an informal definition of what this paper is trying to get at, followed by a very stripped-down explanation using some basic computer theory to explain and summarize this paper. Afterwards, we’ll take a look at some critiques I have about this theory.

## Summary

### Informal

To start summarizing this paper, we need to understand what the core idea and rational is for it. First off, think about how, as humans, we perceive the environment around us. You should typically understand that we perceive the world around us, and it’s pretty logical to then conclude that the world around us, as we perceive it, would be accurate? The Interface Theory of Perception (or ITP), was created in order for us to realize that actually *none* of the things that we see are truly, 100% accurate to the world around us. It is based on the evolutionary fact that different species have different inherent abilities for perception, and in competition between species for resources, it’s imperative (according to Darwinism) for a species’ survival, that it has good and accurate perception of the environment. Otherwise, it will eventually be outcompeted and die out, and those genes will not be passed along, only the genes of the beneficial adaptations will get passed down then. 

However, is it really the case that we need to observe the environment entirely *objectively*, meaning having a one-to-one correspondence of environmental stimuli to our perception of it? It’s definitely been shown that this is not the case, that it is not beneficial to receive all possible stimuli and perceiving all of it. Think about your attention, you really only perceive things that are within your attention, or are unconsciously being attended and monitored and filtered. We eventually evolve *heuristics* or *shortcuts* to be able to handle all that information in a better and more efficient way, almost like ‘life hacks’ that are unconscious. 

Thus, the ITP essentially goes the farthest away from that, instead of perceiving some accurate information, and some in-accurate information, we actually perceive *no* accurate information. But why is that, and where does this come from? Well, with modern technology, we can *simulate* evolutionary games, which pit organisms with survival strategies against others, competing for resources. This is done through Monte Carlo simulation, which is done by simulating these games millions and millions of times, recording the results, and using random sampling and statistical measures to determine, with statistical validity, the best strategies for survival. It turned out that survival strategies which did accurately perceive reality, did not survive and went extinct, whereas strategies that adapted to an optimal *fitness* succeeded. 

Fitness functions are the core premise really, because as competing species, we have goals, states, and actions that we must consider, and need to satisfy our needs to continue procreating and passing on our genes. Thus, instead of focusing on perceiving reality in truth, it’s more important to adapt to your fitness function, regardless of how much you perceive reality. Thus, the ITP suggests that because of these results, it then follows that it would be more adaptive to not perceive reality, but instead interpret it through some species-specific “interface”.

This “interface” is basically the way we perceive and interact with the world around us. The paper itself has a really good analogy, and that is to computers. Computers are almost entirely controlled by their *interfaces*. Think about how you interact with your computer desktop, you have all these windows, icons, and buttons. However, that is 100% not how they are truly represented in the computer. This interface is a way for us to abstract and obscure the complex details of how computers work, into digestible buttons and icons. Also, think about the language we use with computers, we can perform operations on computers. But within the interface of a computer, we cannot express operations on the objects within the computer, through non-computer language, and transversely, cannot explain how computers work through non-computer language. How could you possibly explain the operation of editing a file, through language meant for ballet, or for painting?

Thus, this interface is an adaptive mechanism that we construct as we experience the world to better absorb information, instead of receiving all the environment at once. It makes us better at surviving because we can take in useful information for survival, with great efficiency and speed, and be able to make faster and better decisions, which lead to our survival. This is entirely what the Fitness-beats-Truth (FBT) theorem is about.

A core piece about this theory is also that we have invented and evolved our own interfaces through the process of evolution, so we have invented the world around us as we perceive it. Since we perceive inaccurately, and through our lens entirely, and our entire world is within ourselves, then how do we know what is real?  And since we don’t know what the real world is, how can we explain how we are able to achieve the scientific precision of NASA, and see the quarks and bosons from CERN? The invention of symmetry is the explanation, because since we have constructed this arbitrary interface that is specific to our species, then we are able to flawlessly operate within this interface. More specifically, we perceive ourselves in 3D space, operating with 1D time, and can coordinate within that specification, not anything lesser than it, nor anything more. Think about 4D space, it’s incredibly hard to imagine, impossible even. We cannot truly visualize 4D space, nor 2D space, because we operate and perceive in 3D. We can imagine and understand through math. What about light? We only see from a tiny part of the spectrum, the visible light spectrum, even though there is so much radiation and light passing through us, we do not, nor cannot understand what that would look like. This is entirely because we have evolved to *not* see it, because it was not adaptive for survival. The invention of symmetry is an important model to understand about this theory because it solidifies the fact that we only see ONE reality out of the infinitely many that occur, and through a digestible lens that allows us to even construct images from complex 3D environments through the properties of shapes. Thus, we have evolved our interface to, again, serve our fitness function, and we work and construct concepts within that interface’s language.

Overall, this theory posits the fact that the interface that we interact with, to create action in the world, has constructed a language and concepts as a result, that we cannot use to explain the real world, simply because we do not observe the real world as it truly is. It posits that we only see the world through our own interface, and thus cannot make the unbiased conclusions, such as what happens when we cannot observe an object. It is the attempt to answer the ultimate question of how to reframe our languages to describe reality, instead of using ‘space-time’ and ‘cause’. In this view, there is no causality, because we cannot truly observe the reality of the components of that interaction, and cannot, therefore, arrive to the conclusion that one noumenon **caused** a phenomenon.

### Formal

To spare you the despair of hardcore math and theory, I will skim this formal section and start where it really should have started, and skip the rigorous details of math. First, I must explain some basic theory, such as sets. Sets are collections of objects, we can represent anything with sets, but particularly, we want to represent the phenomena of the world. We want to represent the world and everything within it that we care about. Specifically, the world of total reality, the world of objective reality, which is without the perceiver, and the organism which perceives. We also have a set of experiences that the organism has, which is a subset of the organism itself. Referring to algebra, functions are simply mappings of some input set, to some output set. We can generalize this idea and map objective reality events to organism-specific experiences, essentially saying that when an organism's experiences, come from objective reality. This is a perceptual strategy, which we can modify and add combinations of input mappings to other combinations of output mappings to create strategies for survival.

Through common sense, we can also have combinations of experiences, like "hot or cold", or "wet and cold". Every experience must also have a complement, such as "cold" and "not cold". This represents an important property of the collection of events, that we can combine, and complement experiences to create new ones, which are also experiences themselves. For instance, an interface strategy is one in which the dimension of the input does not mean that the output is of the same dimension. Thinking about that more generally, an interface can transform the space it receives to a lesser degree. Importantly, critical realist strategies are interface strategies that do not change the structure of the mapping, meaning that if we receive 3D space as input, we perceive 3D space. Because all critical realist strategies are interface strategies, not all interface strategies are critical realist, meaning that they are strict interface strategies. These strategies DO NOT represent objective reality truthfully.

Why is this important? Because we can now mathematically define exactly the different types of strategies for Monte Carlo simulations of evolutionary games. Thus, we see through previous research in this field that strict interface strategies always outcompete any strategies that perceive reality as it is, and do it much better. Thus, it is much more beneficial to abstract the incoming information, as the world you compete in becomes more complex.

In the realm of computational theory, we can model any algorithm using Turing Machines according to the Church-Turing thesis, which has never been disproven. What is a turing machine? This is a bit above me, but essentially, we can reduce any model of finite operations into some machine that has states, accepts input, and has set states that act as a start, and those which are 'accept' states. We have some mapping present that allows us to understand from each state, and for every combination of input for every state, where we can go. Essentially, this is a very abstract way to describe some computation, like adding numbers, or keeping track of things. This is precisely what a perceptual agent is defined as based on the sets of the objective world, and the set of perceptual experiences. However, as perceptual agents, we do not just passively absorb information, we make decisions and act on them, based on the available actions. Perceptual agents (us), are essentially reduced into the simplest form, a machine, which has set mappings between perceptions, actions, the states of reality of input, and probabilities. It distinctly ties in classical Bayesian decision theorems, which use probabilities, cost, and benefit calculations to act optimally. These machines model some function of our perception, like motion correspondence, or structure-from-motion. Because these machines are also modular, they can be combined together and simplified. 

Essentially, it is a reductionist view that we are machines that have adapted an interface to observe reality in a better way, and have mappings of actions, decisions, percepts, and states that act to survive based on this filtered input. This is exactly why the hardest problems are too difficult to solve, because we cannot even begin to gnaw at the core concepts without the correct language, which we have not developed because of our interfaced reality. Computationally speaking, maybe this problem IS too hard to solve, as there are specific classes of problems that are just unsolvable. 

## Critiques

- Not deeply explained and operationalized as it should be, for the scope and the audience that would be interested in understanding this theory
    - As much as I would like to say I’m very good at linear algebra, or computational theory, even I struggled with some concepts, despite having rigorous courses about computational theory and graphs and domain mapping.
    - The paper is a remarkable 33 pages, but it really should be longer, and have a lot more operationalization of the models that it uses to support the theory. This is more of a personal critique rather than a content one, because the appropriate audience would then be very slim, as the intersection between high level computer scientist and high level psychologists are far and few between. It would really benefit the theory if it were to fully explain how concepts of graph planarity and vision worked to support the theory, rather than just assume we understand the concepts themselves, or the relationship between them.
- The purpose of the paper is very much about ‘throwing away the papers’ and reframing our language to better suit problems about reality
    - it does not suggest any better methods other than very abstract quantum wave functions and computations
        - which may be a nod to how difficult this problem is, and perhaps how impossibly it would be for it to be adopted.
    - it almost feels like the “all humans are selfish” argument, in which there is always a way to show that humans are selfish, no matter their selfless intent or behaviour. it’s very difficult to put into words, but the overwhelming nature of this theory makes it *too good* at inserting itself and always showing that there is a way that our reality is wrong.
    - this is well illustrated by the idea that ‘there is no causality’ because it lies entirely in the realm of reality, whereas we interact with the environment through our interface, and thus cannot purely understand how one thing causes another - because we do not directly, 1-to-1 see them in reality. This just seems absurd to me, and is a really detoured way to understand that the ITP is meant as a ‘reframing’ of the language that we use to define reality as we know it.
- My last critique, is how I barely have any critiques on the content or the methods themselves. For instance, in the section about vision and shape perception, there is very little mention about how we use binocular vision to perceive depth, and thus can perceive 3D through more than just pure image, and have implicit signals about stimuli. We have an unconscious signal and knowledge about depth. The critique here is that ITP can *always* walk its away around the constructs that we have created. According to the ITP, this depth perception, conscious or not, evolved as a way to guide adaptive behaviour, and thus is a construct that we perceive, but is not a true representation of reality, but rather some beneficial heuristic to navigate within the 3D space we live in.
    - Essentially, it can always reason itself out of criticism, and the content is too abstract to personally make good judgements of criticism based on the depth and scope of the previous literature. Regardless of my experience with computational theory, it seems very reductionist in a time where we know that humans are not living computers, we are more than just machines giving canned responses to the environment that surrounds us.

# EDITS

- use manifold garden footage lmfao
-