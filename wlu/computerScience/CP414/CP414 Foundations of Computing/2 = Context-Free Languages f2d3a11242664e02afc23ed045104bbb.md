# 2 = Context-Free Languages

# Introduction

- Ways of defining languages
    - Finite Automata = Regular Expressions
        - can only express *regular languages*.
        - cannot express non-regular languages, such as $\{0^n1^n|n\ge0\}$.
    - context-free grammars
        - more powerful method of describing languages, which can describe specifically certain features of languages, such as **recursive** structures.
        - as in the use of natural languages, such as the relationships between *nouns*, verbs, and prepositions, and how phrases lead to recursions of these components.
        - very important for defining the grammar of a programming language, which is its *syntax*.
        - the set of languages that are recognized by these grammars are called *context-free languages*.
            - which include all regular languages, and other languages
        - **pushdown automata** are the machines that recognize context-free languages

# 2.1 Context-Free Grammars

Let’s define a context-free grammar, called $G_1$: 

$$
G_1 = \begin{cases}A\rarr0A1\\A\rarr B\\B\rarr \#\end{cases}
$$

- Grammars consist of a collection of **substitution rules**, called **productions**. (each line in that grammar is its own production)
    - productions have a symbol, and string separated by an arrow
    - the symbols are *variables*, and the string contains variables and other symbols called **terminals**.
        - terminals are from the alphabet
    - grammars have **start variables**, that are most often the leftmost and topmost variable (in G1, it is the first A).
    - The variables of G1 are A and B.

<aside>
❕ Describing languages using grammars

Generating strings of a language using the following algorithm:

1) Write the starting variable

2) Find a variable that is within the grammar rules and has been generated already, and replace it with that variable’s definition

3) Repeat step 2 until no more variables remain

</aside>

→ in this way, this exemplifies recursion. As there must be a base-case variable, such that a variable exists that does not define its string in terms of another variable, or itself.

→ Also, we can recursively generate strings with a variables own definition. Take this example:

<aside>
❕ A → 0A1 → 00A11 → 000A111 → 000B111 → 000#111

→ in which the underlined segments are the replacements that are made. We use the string definitions of the variables to replace variables.

</aside>

→ We can also use a parse tree to define the replacements we make in this tree/graph structure.

- where we can trace along the middle of the variables all the replacements we make
- in this example, we start with A’s string (0A1), then replace each A twice, to yield (000A111), then replace A with the string “B”, giving (000B111), then replacing B with its only definition of “#” to get the final (000#111).

- Any string generated by this recursive pattern is part of the *language* of that grammar.
    - represented by $L(G_1)$.
    - which in this example, is the language: $L(G_1) = \{0^n\#1^n|n\ge0\}$.
    - any language generated by a context-free grammar, is a context-free language.
        - context-free languages are closed under these regular operations:
            - **Union** (L(G1) U L(G2))
                - S → S1 | S2, we can generate either L(G1) or L(G2)
            - **Concatenation** (L(G1)L(G2))
                - S → S1S2, we generate both G1 and G2 languages concatenated
            - **Star** (L(G1)*)
                - S → empty | S1(S), we can generate the empty (member of all star classes), or generate any string from G1 and concatenate it with any other string from G1.
- Any strings that are generated by the pattern of the grammar is called a **derivation**
- When generating strings, we can *pipe* and merge rules or substitutions to combine them
    - $A \rarr 0A1$ and $A\rarr B$ becomes $A \rarr 0A1\;|\;B$.

## Formal Definition of Context-Free Grammars

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled.png)

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%201.png)

<aside>
❕ A context-free grammar is a 4-tuple $(V, \Sigma, R, S)$:

1) V is a finite set called the **variables**

2) $\Sigma$ is a finite set, disjoint from V, called the **terminals**

3) R is a finite set of *rules*, with each rule being a variable mapped to a string of variables and terminals

4) $S \in V$, is the starting variable

</aside>

→ if *u, v, w* are strings of variables and terminals, and the rule exists of A → w, then “uAv” yields “uwv”.

→ if u *derives* v, then $u \overset{*}{\implies} v$

→ if u = v, or if a sequence exists for k ≥ 0 such that u1, u2, ..., uk, then $u \implies u_1 \implies ... \implies u_k \implies v$

The language of the grammar is defined as $\{w\in\Sigma^*\;|\;S\overset{*}{\implies}w\}$

- essentially, the strings which are in the star-set of the language, such that strings are derived from that grammar

For example, for the language G1, let’s set up the formal definition:

- $V = \{A,B\}$
- $\Sigma  = \{0,1,\#\}$
- $S = A$
- $R =$  rules of the grammar

→ We can easily, implicitly understand all of the formal definitions just from the set of rules, as each unique variable definition is part of the variable set, the rules are clearly there, the starting variable is the first, topmost variable, and the language (terminals) should be all present in the string definitions of variables.

---

Example: The language of all strings of *properly nested parentheses*:

$G_3 = \{\{S\}, \{a,b\}, R, S\}$

$S \rarr aSb\;|\; SS\;|\;\epsilon$

→ generates strings like “abab”, or “aaabbb” and “aababb”

→ if the terminals were {(, )}, then it would look like:

- “()()”, or “((()))” or “(()())”

---

## Designing Context-Free Grammars

- many context-free languages (CFLs) are the *union* of simpler CFLs.
    - we can construct a CFG for a reducible CFL by constructing the simpler CFGs for each reducible piece, and the merging them with a union of their rules
    - we also must add a rule $S \rarr S_1 | S_2|...|S_k$ for which each $S_i$ is the start variable of each distinct CFG, essentially making it into one concise starting point between all CFGs.
- if the CFL happens to be regular, then we can construct the DFA that recognizes that language, and then convert that DFA into a CFG using this algorithm:
    
    <aside>
    ❕ Converting a DFA → CFG
    
    For each state in the DFA, make a variable $R_i$. Add rules such that $R_i \rarr aR_j$ to the grammar if $\delta(q_i, a) = q_j$, meaning that if state i has a transition to a state j, it is represented in that variable’s definition string by adding the transition symbol as a terminal *before* the variable state itself.
    
    For each accept state of the DFA, add a rule $R_i \rarr \epsilon$.
    
    $R_0$ is the start variable of the grammar, where $R_0 = q_0$, the start state of the machine.
    
    </aside>
    
- Certain CFGs have strings with *two* substrings that are linked. It’s the easiest case to think about trying to have two substrings such that they have the same length. This is something that requires infinite memory, which makes it impossible to construct a *finite* automaton, such as the language $0^n1^n$, because to verify strings in this language, we need to keep track of a possibly infinite amount of 0s, which MUST equal the number of 1s, and vice versa.
    - CFGs that have this pattern typically have rules that look like: $R \rarr uRv$, because they generate strings automatically for which the number of u’s = v’s.
- in complex languages, strings may contain structures that appear *recursively* as part of other, or same, structures.

---

## Ambiguity

- refers to the ability of a grammar to generate the *same string* in several different ways
    - ie. has the same output, for many different inputs.
- if a grammar generates a string in multiple different ways, then that string is *ambiguously* derived in that grammar, and that grammar itself is *ambiguous*. Otherwise, it is *unambiguous.*
- in formal matters, a grammar becomes ambiguous if there exists more than 1 parse tree for any string, ie. an unambiguous grammar has a unique parse tree for each string.
    - this is because derivations may output the same string even when the differ simply by the *order* of which variables are replaced, but not their *structure*.
- To be able to ‘standardize’ this order, we must define a type of derivation that replaces variables in a fixed order.
    - derivations of strings in a grammar are **leftmost derivations** if at every step in the *leftmost* remaining variable is the one replaced.
        - thus, a string is derived ambiguously in a CFG if it has *two or more* different *leftmost* derivations.
            - here, parse tree = leftmost derivation
                - we cannot use normal, unstandardized derivation in this definition because we cannot compare the two.
- Some ambiguous grammars have equivalent, but non-ambiguous grammars that generate the same language
- Some CFLs however, can ONLY be generated by ambiguous grammars, which are called inherently ambiguous

---

## Chomsky Normal Form

- is a format of CFGs that displays them in the most useful, and simplest form. most useful and seen when giving *algorithms* for working with CFGs

<aside>
❕ A CFG is in Chomsky Normal Form (CNF) if *every rule* of is of the form:

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%202.png)

→ where *a* is any terminal, and A/B/C, are any variables, with the exception that B or C are NOT starting variables. Also, it is permitted to have a rule such that $S \rarr \epsilon$, given that S is the start variable. 

</aside>

### Theorem 2.9

<aside>
❕ Any context-free grammar is generated by a CFG in Chomsky Normal Form

</aside>

### Proof 2.9

We can convert any grammar into CNF, because all we have to do for any conversion is change any rules that violate the CNF form into CNF by seperating terminals from variables, and make sure the starting variable has no variables that are itself in its definition. We can also easily eliminate all *empty* rules, and all *Unit Rules* (which just one variable to only one other, such as A → B)

<aside>
❕ Procedure of converting any CFG to CNF

</aside>

---

[https://www.youtube.com/watch?v=h1OSmLSacNA](https://www.youtube.com/watch?v=h1OSmLSacNA) + [https://www.youtube.com/watch?v=o64uz2K2S4E](https://www.youtube.com/watch?v=o64uz2K2S4E)

- Regular grammars are rigid in their rules: var → empty, var → terminal, var → var, or either (var → term+var || var → var+term)
    - we cannot change these rules, otherwise we start getting languages which are not regular, such as $0^n1^n$ for example.
    - this means we cannot represent these languages using a DFA/NFA because they cannot be pumped, they require inf. memory

- **context-free grammars** have only 1 rule: var → string
    - where ‘string’ can be ANY combination of terminal and variable.
    - CFGs are looser in definition, because they generate a larger set of languages, some of which are not regular
    - this means that ALL regular languages are generated by context-free grammars, its just that there are restricted rules on those grammars
        - but not all context-free languages are regular
- Context-free grammars are *4-tuples* composed of:
    - variables, terminal language, its rules, and the start variable
        - with the single rule that [A → x] means that x can be any combination of terminals and variables.
    - when *deriving* or *generating* strings using the grammar’s rules, we use $\implies$to denote the *order of operations* that goes from the start variable to the derived string, and means “yields”
    - A derivation of a string is an order of application of the rules of the grammar, to yield a string that is in $\Sigma^*$, the language expressed by the grammar, meaning that the string must ONLY have terminals, and no variables.
- **leftmost derivations** are when we are deriving a string, and ONLY use the leftmost variable for substitution. This results in always picking the leftmost variable, over and over, until it turns into a string of only terminals, then we can move on to the next leftmost variable.
- leftmost derivations are important, because there can be different leftmost derivations for any string in some cases (not all). In the case where we can derive the same string in different ways, with two different leftmost derivations, we call that string **ambiguous**.
    - this is only the case with different *leftmost derivations* because that implies that different rules are used, and therefore are different ways of achieving the same string, with different rules, and not order (because the order is the same, always leftmost)
- When a grammar can yield some string ambiguously, it is called an *ambiguous grammar* (not the language) because the grammar itself is ambiguous in generation. However, there are grammars which have an equivalent non-ambiguous grammar, generating the same language.
    - A big question, is can we remove ambiguity from a grammar?
        - NO, meaning that there is no algorithm or steps to take that guarantee any ambiguous grammar can become an unambiguous grammar.
    - Or, do languages exist that are **inherently ambiguous?** meaning that ALL of the CFGs that generate it are ambiguous?
        - YES.

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%203.png)

---

[https://www.youtube.com/watch?v=lEsDll4Um7Y](https://www.youtube.com/watch?v=lEsDll4Um7Y) (Chomsky Normal Form)

- A ruleset for ALL grammars (regular AND context-free) that gives a lot of information for us to be able to understand what a grammar does
- because we can “obfuscate” and “minimize” grammars by just jamming all the variables and terminals into the start variable.
    - The ruleset is three simple rules:
        1. The only variable that can substitute $\epsilon$ is the start variable
        2. A → a
            1. variables can define only single terminals
        3. A → BC
            1. or, variables can define exactly two other variables
            2. AND, B nor C cannot be the start variable
- Any grammar can be converted into CNF, there is an algorithm to do so. It’s also extremely important that no step within this algorithm *changes the language* that the grammar generates.
    1. Ensure that the start variable (S) is not within any other variable’s definition
        1. we can do so by creating a NEW starting variable, that leads to the original start variable. This way, no existing variables will reference the new starting var.
            1. this does not change the grammar because it does not allow for generating other terminals, it only generates the original starting variable, and it does not change how we derive the original language, other than an intermediate step at the beginning.
    2. Changing definitions of variables which generate the empty terminal ($\epsilon$).
        1. This is harder, because just removing these definitions can change the language, and, since variables can define other variables, may change the way other variables generate their definitions.
        2. The way to workaround this is to find all the **nullable** variables which either i) directly generate empty ii) or generate ONLY variables, and each of those variables is nullable. This essentially means that a nullable in some way, can become the empty string either by the empty string itself, or its generating variables can all generate the empty string
            1. if [A → xBy] is a rule, and B is nullable, we can add the rule [A → xy]
            2. if S0 is nullable, add the rule that it itself generates the empty string (because of the rule that ONLY the start var. can generate empty)
    3. All variable definitions that derive either i) terminals, ii) terminals + variables iii) > 2 variables are not allowed by CNF
        1. To first tackle this, we want to try and work around all unit rules, where one variable is defined in terms of another, such as A → B. These rules don’t create new terminals in the string which change the string, they only replace other variables.
            1. To eliminate unit rules, take this example: A → B, B → C, C → ab. Then we can see that each A, B, and C can generate ‘ab’. So, an easy way to eliminate unit rules is to replace them with the ‘shortcut’, effectively cutting out the middle-man of the other variables. So they become A → ab, B → ab, C → ab. This does not change the language because each of them generate exactly the same thing they have been generating.
                1. The proper format of fixing this rule, is to created a *directed graph* of Nodes being variables, and their directed edges from A → B if and only if there exists a grammar unit rule such that A → B. If there is a directed path between any two variables, we can deal with all of the occurrences.
                2. In this case, we can look at ALL rules which contain the variable A, because it connects to B, and for each of those rules that contain A, as in X → uAv, we can add a new rule that declares X → uBv. This does not change the language because it cuts out the intermediate step of subbing in B for A, and uses B directly.
        2. The next type of bad rules is the *mix* of terminals and variables, as in A → xB, or any combination of vars and terms.
            1. This is done by ‘breaking up’ the mixes into their individual rules, that each follow the (almost CNF) rules:
                1. either var → single terminal
                2. or var → multiple vars.
                    1. with these two rules, they are not exactly in CNF because CNF limits var → vars to a definition of only/exactly 2 vars, but this is close enough.
                
                i. to break up these mixes, we need to first create NEW rules for each terminal, with new variables, whose only rule is to generate that single variable. Then **replace** those terminals in any definitions, with that variable. Effectively creating new variables for only terminals, and then once we replace all terminals with variables, will create variable definitions with only variables.
                
        3. The last type of definitions we need to fix are *long* variable definitions. Because we eliminated unit rules (len. 1), we need to break up “long” rules which have ≥ 3 variables, because any rules which have = 2 variables exactly, are in CNF and don’t need to be fixed.
            1. One of the many ways to do this, is to break up these long variable definitions in the same way we did terminals, creating entirely new variables and rules for each, but this time, we take away the first two variables in that long definition, and generate that pair using a new variable, then replacing those two variables in the original long rule with the variable.
            2. This gives us some rule now, like $A \rarr Y_1X_3...X_m$, where $Y_1 = X_1X_2$, and still generates the same language because it introduces a new middleman. Thus, the rule for A now has 1 less variable in it, so we repeat on A until A has broken up each variable pair.
                1. Let’s take the example that A has 6 vars, then breaking and replacing those 6 into 3 new vars would mean that A has 3 vars, as in $A \rarr A_1A_2A_3$, so this would just mean that we create 1 more new var such that $A_{12} = A_1A_2$ then $A = A_{12}A_3$, and thus A now has only 2 vars.

---

# 2.2 Pushdown Automata

PDA’s are similar to NFAs, except PDA’s have an extra component called a **stack,** which is a way to expand memory beyond the finite.

- stacks allow PDAs to recognize some nonregular languages.

PDAs have an equivalent amount of power to CFGs, and we can use this fact to prove that some languages are context-free.

- thus, we can either give a CFG that **generates** that language, or a pushdown automata that **recognizes** it.

In-order to define what a pushdown automaton looks like, we need a different way of inputting symbols:

- the control represents the states and transition function, with the input coming into the machine through a tape-array. the arrow specifies the input head, at which it points the next input to be read into the machine.

- the FA turns into a pushdown automaton once we add a *stack,* which is very simply, a stack data structure.
- a PDA can write symbols onto the stack, and read them *later*, and is called a *pushdown* machine because adding onto the stack *pushes down all other symbols on the stack*.
    - adding onto the stack is called **pushing** and only happens on the *top*, while removing from the stack also only happens on the *top*, but is called **popping**.
    - thus, we can only access written elements of the stack in the order from the top.

Stacks are very special, because they hold an *infinite* amount of information, and can thus recognize some non-regular languages because of it. We can represent the language $0^n1^n$ with this stack like so:

- reading input, whenever a 0 is read, push it onto the stack. As soon as a 1 is seen, pop a 0 off the stack for each 1 that is read. If reading the input is finished exactly when the stack is empty of 0s, accept the input.
- However, if the stack is empty, but the input hasn’t finished, that means # 1s > # 0s. Or, if the stack still has elements inside of it and the input is done, then # 0s > # 1s.

→ this illustrates a key idea, that pushdown automaton are closer to data structures, and thus are more ideal for being able to represent certain situations of languages.

<aside>
💡 PDAs may be *nondeterministic*, and are NOT equivalent in power to deterministic PDAs.

(unlike NFAs and DFAs which recognize the same *classes* of language (regular ones))

</aside>

→ since NPDA (nondeterministic PDAs) can recognize certain languages that no deterministic PDA recognize.

- however, NPDAs are equivalent in power to CFGs.

![Finite Automaton](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%204.png)

Finite Automaton

![Pushdown Automaton](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%205.png)

Pushdown Automaton

---

## Formal Definition of Pushdown Automatons

- Since PDAs are identical to finite automatons, except the stack, they are defined in a very similar way.
- Stacks are just *data structures* that fulfill a purpose, to hold elements of input for later reading, however, the stack may use a *different alphabet* than the input alphabet.
    - As we also have a stack, we now also need to define its behavior on input through the *transition function*.
- The transition function is now based on the domain of:

$$
Q \times \Sigma_\epsilon\times \Gamma_\epsilon
$$

- → which means that the current state, the next input symbol, and the top symbol of the stack determine the next state of the pushdown.
    - (remember that $\Sigma_\epsilon$ is the alphabet union’d with the set containing the empty string)
    - which means that the empty symbol $\epsilon$ is valid, and reading the empty symbol after input, or without reading from the stack at all, causes the machine to change states given there exists a transition from the current state to another for this empty input.
- We must also understand that in a single move, the machine can transition to a new state, and write a new symbol on the top of the stack.
    - this is indicated by *returning* a member of Q together with a member of $\Gamma$.
    - and since non-determinism is allowed, the transition function looks like: $\delta: Q\times\Sigma_\epsilon\times\Gamma_\epsilon \rarr \Rho(Q\times\Gamma_\epsilon)$.

Putting it all together, this is the form of the pushdown in formal notation:

<aside>
💡 6-tuple: $(Q, \Sigma, \Gamma, \delta, q_0, F)$

- Q is the set of states
- $\Sigma$ is the input alphabet
- $\Gamma$ is the stack alphabet
- $\delta: Q\times\Sigma_\epsilon\times\Gamma_\epsilon \rarr \Rho(Q\times\Gamma_\epsilon)$ is the transition function
- $q_0 \in Q$ is the start state
- $F \subseteq Q$ is the set of accept states
</aside>

→ it computes by accepting an input *w* if *w* can be written as a sequence of letters from the input alphabet, and there exists a sequence of states, each of which exist in the set of possible states, Q, and also now the strings which exist in the $\Gamma^*$ set.

- essentially, we must also now consider the different possibilities of the stack based on the stack alphabet, of which there are infinitely many in the stack alphabet’s *star* set. The set of strings possible in the star-set of the stack alphabet represent the stack’s contents that the machine has on the accepting branch of the computation.

The Pushdown must satisfy 3 conditions for an input string to be **accepted**:

1) $r_0 = q_0$ and $s_0 = \epsilon$, meaning that the machine starts in the start state, and starts with an empty stack

2) for any i, there exists a tuple of (state, stack) which is an element of the transition function given a (state, input symbol, stack symbol).

- essentially meaning that the machine moves deterministically along the transition function based on the state of the stack, the machine state, and the next input symbol.

3) $r_m \in F$, meaning that an accept state occurs at the end of the input

- since the pushdown is non-deterministic, this also means that if any computing branch is in an accept state, the entire machine accepts as a whole

---

- Example: $\{0^n1^n|n\ge 0\}$ in pushdown automaton
    - Q = {$q_1, q_2, q_3, q_4$}
    - $\Sigma = \{0,1\}$
    - $\Gamma = \{0,\$\}$
    - F = {$q_1,q_4$}

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%206.png)

- A major new idea here, formulated with the formal notation of the transition function, is that for each input, there needs to be options for each stack element at every
    - how you would ‘read’ this would be by the format $a,b \rarr c$. This represents that we read *a* from the input string, *b* on the TOP of the stack, and the element to place on top of the stack in result.
        - essentially, when reading *a* from input, if the top of the stack is a *b*, then we push onto the stack the element *c*
        - also, any one of these three can be *empty*
            - if *a* is empty, then we can transition without input
            - if *b* is empty, then we can transition without a specific symbol being on top of the stack
            - if *c* is empty, then we transition without pushing onto the stack.

![PDA of 0^n1^n](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%207.png)

PDA of 0^n1^n

- In the formal definition of a PDA, there is no way to test for an empty stack. However, this PDA gets past this by placing a special ‘empty’ symbol on the stack, $.
    - $ is only placed on the stack when it is empty, and if $ is read, then we know the stack is empty.
        - and since the stack language does not need to be the same as the input language, we can use any arbitrary characters to denote this.

---

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%208.png)

- Example of $a^ib^jc^k$, where i = j OR i = k, and i,j,k ≥ 0.
    - this means there must be at least 1 of each a, b, and c’s.
    - and that the number of a’s must match the number of b’s, OR the number of a’s matches the number of c’s.

- The non-determinism of this machine is what makes it possible to recognize this language, because we cannot know whether to match the a’s to the b’s OR the c’s, so non-determinism allows us to have two different branches of computation that **guess** in either branch.
    - When the machine starts, we know the stack is empty, and adds the empty character ($) to it and arrives at q2.
    - at q2, it can either loop on input ‘a’ and add ‘a’ to the stack.
        - at which it then branches to q3 and q5, which is the point at which it “guesses” which character to match ‘a’s to.

---

Example 2.18: $\{ww^R|w \in \{0,1\}^*\}$

- $w^R$ represents the *reverse* of the string w.

This example can be very elegantly presented by understanding the use of non-determinism well. In this solution, we want to “guess” whether the middle of the string $ww^R$ is reached, which is the point at which the substring *w* is reversed.

Since we have the very useful stack as well, we are essentially creating a stack that has the *w* substring inside of it, which is the purpose of q2, to add onto the stack. Once q2 adds onto the stack, we can arbitrarily guess that the middle of the string is reached, by branching off non-deterministically, and using that new instance’s stack version to pop elements off, and continuing to keep that q3 cycle alive if the current input of the string matches what is stored in the stack, which is the substring *w.*

The machine only accepts if the branch is kept alive, meaning we have a reversed first-half as a substring in the second half, and we reach the end of the stack, so we 100% guarantee that the current input string is of the form $ww^R$.

---

## Context-Free Grammars equivalence

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%209.png)

- the definitions of context-free grammars and pushdown automata have equivalence in their power to describe context-free languages, and in-fact, they describe the same languages. Similar to how DFAs can describe the same class of languages as NFAs do, regular languages.
- We know this, because there is a method of converting any context-free grammar into a pushdown automaton.

<aside>
💡 Theorem 2.20

A language is **context-free** if and ONLY if some *pushdown automaton* recognizes it

</aside>

<aside>
💡 Lemma 2.21

If a language is *context-free*, then some pushdown automaton recognizes it.

PROOF:

If we have a language A that is a context-free language, then we know it has a context-free grammar that describes it, by the definition of the relationship between context-free grammars and the language that it generates. Then, we need to define how to construct an equivalent pushdown automaton (PDA) from that grammar, which represents the same language.

PDAs work in the opposite way that CFGs do, instead of *generating* a string that belongs in the language (as regular expressions could), PDAs must start with the derived string, and *test* if they exist in the language. This is identical to the problem of trying to find a valid *derivation* for the input *w*, which would be the string generated by the grammar.

To construct a valid PDA, we can utilize non-determinism to guess **the sequence of correct substitutions to make. At each step we can select a rule for a variable’s substitution and non-deterministically branch and guess.

Since PDAs also have a stack, we can use this stack to store *intermediate* strings, and making substitutions using this stack. Eventually, when the PDA’s stack contains *no variables* and only terminal symbols, then we have derived a string using the rules of the grammar, and then we must evaluate whether or not the derived string matches the input string.

However, storing intermediate strings as a whole in the stack will not work exactly, because the PDA can only effectively use the top of the stack to make decisions, so to work around this, we keep parts of the intermediate string whose symbols start with the first variable, because any terminal symbols appearing *before* the first variable can be directly matched to the symbols in the input (match) string.

1.  start PDA by adding $ onto the stack
2.  repeat these steps infinitely:
    1. if the top stack is a variable symbol, nondeterministically choose a rule for that variable and substitute it.
    2. if the top stack is a terminal symbol, read next symbol from input and compare it, if they match: repeat, else, reject on this branch.
    3. if the top stack is the empty symbol $, enter accept state.
</aside>

<aside>
💡 Lemma 2.27

If a pushdown automaton recognizes some language, then it is *context-free*.

PROOF:

If we are given a PDA, we must show here that it can also then be converted into a CFG that generates all of the strings that the PDA accepts. The grammar we construct must generate a string if that string causes the PDA to accept.

The idea for this construction proof is that for each *pair* of states in the PDA, the grammar will have an equivalent variable $A_{pq}$, which represents the variable that will generate all strings that can take P from the state *p* with an empty stack, to *q* with an empty stack.

Which also means that these strings allow progressing the state from p → q regardless of the stack contents.

We must modify the PDA to have a general form with the following features:

1. It has a single accept state
2. It empties the stack before accepting in that state
3. Each transition must either push a symbol onto the stack, or pops one off the stack, but not both at once.
    1. to implement this third requirement, we can replace any non-stack changing states with a sequence of two arbitrary stack moves, such as the first which pushes, then the second which pops that arbitrary symbol, effectively leaving the stack unchanged, but still following by the rules.
</aside>

By proving that pushdown automatons recognize the class of context-free languages, and the context-free grammars that generate those languages are then 1-to-1 equivalent in the sets they recognize, we can also learn about how context-free languages are related to *regular* languages.

- If context-free languages require a stack to be recognized, then we can think about how regular languages are languages that do not require stacks, and are similar to context-free languages that ignore their stack entirely. Thus:

<aside>
💡 Corollary 2.32

**Every regular language is context free**

</aside>

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%2010.png)

---

[https://www.youtube.com/watch?v=ZImtQBMSW_Y](https://www.youtube.com/watch?v=ZImtQBMSW_Y) (CFG → PDA)

- Given a CFG, we can always convert it into a PDA because they are equivalent (as shown by the previous theorems). However, this is a more exemplified look into how we can construct the PDA.chom
- Given that we always want the PDA to accept only when it has read all of the input AND the stack, we need to add as the first transition
    1. adding that special $ (empty stack) character onto the stack without popping or reading.
    2. Next, we know that the CFG always starts with the starting variable, so we can also add the starting variable onto the stack without popping or reading.
    3. Then we have a central “loop” state that should read and pop depending on the input. This loop state will have a self-loop for **each** terminal character, popping the terminal character itself from the stack, and pushing nothing.
    4. When in this loop state, we only want to accept if we read nothing (input is done) AND we have on top of the stack the empty stack symbol.
        1. this signifies that we have matched a string that is generated by the CFG, and matched it entirely, character-by-character, in the input-stack fashion.
        2. meaning that for each terminal character in the stack, it must match EXACTLY as the input.
            1. essentially, we “generate” the string in the stack and compare it to the input, we push to the stack the substitutions we make.
    5. The essence of this construction is that the SELF loop from loop → loop state controls the comparison between stack and input, and we then have a secondary loop cycle (that goes to other states and not loop itself) that will not read from input, but rather pop and push from/to the stack, generating the string in the stack.
        1. also, because this is a stack, we want to do this in **reverse** order. So if we have the rule (S → aBc), then pushing a→B→c results in popping c→B→a. So, we push c→B→a to pop the original.
        2. This is done FOR EACH rule in the grammar, each rule has its own state-cycle from loop → intermediate generating rule → loop.
- At a high-level, this machine has some important patterns that are required for the reverse construction, converting a PDA → CFG. It must not have any transitions that both READ and POP at the same time, but always accept only when the stack is empty, and only has 1 accept state.

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%2011.png)

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%2012.png)

---

[https://www.youtube.com/watch?v=X0nrYIVGs3M](https://www.youtube.com/watch?v=X0nrYIVGs3M) (PDA → CFG)

- Converting a PDA into a CFG requires special formats of PDAs, because if you examine the CFG → PDA example, it has some special properties that make it really simple once we convert a CFG → PDA, these are the properties:
    - There is only a SINGLE accept state
    - the stack must be empty when the machine accepts (either naturally, or it is forced to be emptied out)
    - no transitions must have both read and pop operations.’
- A helpful idea for this is imaging the height of the stack graphed over the number of transitions.
    - The important part of this visualization, is that the CFG that we construct must generate the same language that is recognized by the PDA, and because of this, the PDA must match the start and end operations, like pushing $ at the start and popping $ at the end.
    - Thus, this forms a mountain/hill of stack height, because once the stack is at its maximum, this represents the entire portion of a string being generated within the stack, and it then matches stack-to-input, removing from the stack until it fully matches 1-to-1 symbols, and moving forward if they are identical.
    - Thus, every start and end of each mountain must be matched because of this matching logic, matching stack to input
- Once all THREE conditions are fulfilled on the format of the PDA, then we can proceed to construct a grammar out of that PDA’s behavior.
    1. First, we find all the variables that are within the CFG based on the states of the PDA. We construct a variable $A_{pq}$ for *each* p, q states of the (modified) PDA
        1. an arbitrary $A_{pq}$ is the variable that generates all strings going from state p → q, with the **same stack height**
            1. *p* and *q* do not need to be different, sometimes they need to be the same.
        2. The start variable, by this logic, is then $A_{q_0f}$, for which is the string generated from state $q_0$ in the PDA to the (single) final state *f*.
        3. We also then need to find the *base case* of the subproblem (if you remember stack height vs. input, we get matching mountains within mountains, which means subproblems within problems)
            1. The base case is at a ‘peak’ of these mountains. We know it to be always true that going entering a state *p* and staying in that state *p* can result in the empty string, because no string is generated by the action of simply doing ‘nothing’, so we have constructed a base-case for the fact that each state has a grammatical rule that creates the empty string.
        4. Thus begins the three essential algorithms for describe the rules of the grammar (CFG) we are constructing:
            1. For every state in the PDA, add a rule to the grammar such that $A_{pp} \rarr \epsilon$ (Type III (text))
                1. is a **base case** for each state
            2. Then, based on figure 1, we want to get to different states with the *same* stack height, we can go from p → r, and then r → q, so we can go from p → q by taking *r,* without changing the stack height. Thus, this is the second rule, for every three states ($p,q,r \in Q$), make a rule $A_{pq} \rarr A_{pr}A_{rq}$ in the grammar’s rules.
                1. relies on the **states** of the PDA
                    1. and is a case for which we have ‘two mountains’, as it achieves the same stack height THREE times, meaning that there are three places in which they are at the same stack height, and we can traverse p → r → q.
                2. p, q, r do not have to be different
            3. This last rule relies on the transitions themselves, and applied for ‘one mountain’, which means that the same symbol must be pushed onto the stack, and popped off the stack at the start and end respectively. However, we can guarantee the start and end, but not what happens in the middle, because that now matters. So, if we look at figure 2, we can see that to describe the transitions of the PDA in grammar rules, then we need to describe this transition from p → q through the use of r → s, because r and s have the same stack height. But since we don’t know what r → s generates, we can use the variable $A_{rs}$.
                1. Thus, for each ($p,q,r,s \in Q$) and $u \in \Gamma$, with reading transitions $a,b \in \Sigma_\epsilon$, then add a rule $A_{pq}\rarr aA_{rs}b$ in the grammar rules.

[https://www.youtube.com/watch?v=GaQPxCgSAMQ](https://www.youtube.com/watch?v=GaQPxCgSAMQ) (PDA → CFG Example

- In order to modify the PDA to have the three special rules apply, we do these things:
    - create NEW accept state, such that there is only 1 at all, getting to it from the original accept state. Also creating a NEW start state that goes to the original accept state.
        - The transitions out of the NEW start depend on a new, special stack character, like ‘#’ for example that is used to symbolize the NEW end of the stack. Thus, we must also use this new special # character as a way to enter the NEW accept state.
    - create a self-loop on the original accept state, such that it will *empty* out the entire stack before it accepts, using the new # character to symbolize the new end-of-stack.
        - this means that the self loop transitions when nothing is read, and only pops each character that would be on the stack.
            - for the example in the video, we’re then popping either the old special char. $, or ‘0’, and not popping ‘1’, simply because we do not push 1 onto the stack ever, because we are always comparing 1’s to 0’s which are stored in the stack.
    - every transition **has either a *push* move, or a *pop* move**, but not both at the same time. IF there are any transitions which do both at once, we can separate the single transition into two transitions with an intermediate state, the first that pushes and the second that pops.
        - Additionally, for any totally empty moves ($\epsilon, \epsilon \rarr \epsilon$), we can create arbitrary transitions that make them meaningful by choosing some random (non-special) stack character to push first, then pop in the double transition format that we are looking for.
- this video shows how converting from a PDA → CFG can be very algorithmic, given that the PDA is in the CFG format, which means it has only a single accept state, the stack is empty (and is forced to be emptied out) by the time it accepts, and no transition reads and pops at the same time.
    - Once we have these three conditions guaranteed, the construction of the CFG depends entirely on the *combinations* of states and transitions that exist in the PDA.
    - For example, the video shows the modified, correct PDA to have 9 states, and we have a null rule created for each state (type III)
        - $A_{pp} \rarr \epsilon$
    - next, we create each *pair* of variable states that requires three states each.
        - $A_{pq} \rarr A_{pr}A_{rq}$, for which they match up this way.
            - one important thing is that we can do this algorithmically, but not all variable grammar rules will be useful. Because it depends on the transitions between each state to make sense, otherwise, it generates nothing.
            - For example, if we create the variable $A_{q_0q_3} \rarr A_{q_0q_8}A_{q_8q_3}$, then that implies that to generate this variable, we take the transition from q0 to q8, then q8 to q3, but the PDA has no backwards connection that way, so it generates nothing.
                - These rules are technically correct, because they are empty, but are not the minimal representation of the CFG.
                - based on the number of combinations, if we have 9 states, and we need to choose three of them, then there are $9\times9\times9$ possible combinations for variables. Thus, most of them will be useless because this PDA is not complete graph of connection between all other nodes.
    - For the last type of grammar rules that we need to construct, we use all the transitions which push and pop the *same* symbol in this format:
        - $A_{q_0q_8} \rarr (q_0\rarr q_1 read)A_{q_1q_7}(q_7\rarr q_8 read)$.

![Figure 1](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%2013.png)

Figure 1

![Figure 2](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%2014.png)

Figure 2

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%2015.png)

---

# 2.3 Non-Context-Free Languages

Just like how we can prove that there exists non-regular languages, and used the pumping lemma to directly show that certain languages have strings *which cannot be pumped*, meaning that those languages are non-regular, we can use the pumping lemma to also show that there exists strings in languages which are “context-free”, which cannot be pumped, therefore, those languages are not really “context-free”

Similarly, every context-free language has a special value, the **pumping length** that follows the rule that all longer strings (than *p*) can be “pumped”. In this case, the importance of this pumping length is that the string can be guaranteed to be divided into **FIVE** parts, not three. This is so that the **2nd** and **4th** parts can be *repeated* together any number of times, and the resulting string remains in the language.

<aside>
💡 Theorem 2.34

**Pumping lemma for context-free languages**

if A is context-free, then there is a special number, *p*, where if a string *s* is any string in *A* that is *at least* then length of p, then s may be divided into 5 pieces, $s = uvxyz$ that satisfy the 3 conditions:

1. for each i ≥ 0, $uv^ixy^iz \in A$
2. $|vy| > 0$
    1. either *v* or *y* are not the empty string
3. $|vxy| \le p$
</aside>

PROOF:

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%2016.png)

Given a CFL ‘A’, and G is the CFG that generates it. Then, we must show that any long enough string *s* in the language can be *pumped* to remain in the language.

If the string ‘s’ in A is very long, and is derivable from G, it has a parse tree.

If ‘s’ is ‘very long’, then its parse tree must be ‘very tall’, meaning more objectively that it has a long path from the start variable (root) to the last terminal substitution (leaf).

Because it is very long, then some variable ‘R’ must repeat due to the pigeonhole principle

Referring to the parse-tree image, we can see that a string *s* with a repeating symbol can yield different parse trees to yield different string derivations, because we can add more R repeats, or remove them.

- if we look closely, those 5 pieces, uvxyz, are present, and adding or removing repetitions of R causes repeats of the *outer* terminals, which are the 2nd (u) and 4th (y) pieces.

To sufficiently prove this, we must also look more technically at the *tree*. If we limit the number of symbols that can appear in the variable definition, then we know that nodes of variables can have *no more* than **b** **children. There are at most *b* leaves within 1 step from the root, $b^2$ leaves within 2 steps, etc, leading to $b^h$ leaves within *h* steps.

So, as the height of the parse tree is at most *h*, the length of the string is AT MOST $b^h$, thus, any generated strings that are of length $b^h +1$, then we can guarantee that its parse tree is at least $h+1$ nodes high.

If $|V|$ is the number of variables in G, then *p* can be the length $b^{|V|+1}$. If we have a string *s* is in the language, and its length is at least *p,* then we know its parse tree to be at least $|V| + 1$ high, because $b^{|V|+1}\ge b^{|V|}+1$.

We then must choose $\tau$  to represent on of *s*’ parse trees, which is chosen to be optimally the one with the smallest number of nodes. Thus, we know that $\tau$  must be at least |V| + 1 high, and the longest path from the root to the leaf is at least |V| + 1 high, meaning that the path has AT LEAST |V| + 2 nodes, with one being at a terminal, and the others at variables, meaning there are |V| + 1 variables (the +2 comes from the terminal node).

Thus, if the grammar has |V| number of variables, having |V| + 1 variables in the parse tree means that a variable *must* repeat. Thus, we can divide *s* into $uvxyz$, with each occurrence of R having a subtree underneath it, which generates the substring $vxy$, and the last leaf node generating only $x$. Replacing smaller subtrees with larger ones repeated gives parse trees for the string of the format $uv^ixy^iz$, at each i > 1, and replacing larger ones with smaller trees gives $uxz$. (condition 1)

To satisfy condition 2, *v* and *y* must not be either empty, otherwise this would mean that the parse tree would have *fewer* nodes than $\tau$, but still generates *s*, but is not possible based on the way we picked $\tau$ to be the parse tree with the smallest number of nodes.

To satisfy condition 3, $|vxy| \le p$, which means essentially that the parse tree for the string *s* has an upper-occurrence of the variable R that will generate $vxy$. We chose R to be a variable such that both occurrences fall within the bottom |V| + 1 variables on the path to deriving that string *s*, while also choosing the longest path of the parse tree. This means that the subtree in which R generates $vxy$ is AT MOST |V| + 1 high, so trees of this height can generate strings at length AT MOST $b^{|V|+1}=p$.

---

Examples of pumping lemma:

---

# 2.4 Deterministic Context-Free Languages

Like with DFAs, they are not as powerful in capturing languages easily. Nondeterministic FAs are much more robust in constructing a machine that can recognize languages.

Thus, we can now understand how *deterministic* PDAs are made because of the parallel between NFAs and DFAs. Certain context-free languages cannot be recognized by **deterministic pushdown automata** (DPDAs), because they require non-determinism. And any language that can be described by DPDAs are called *deterministic* context-free languages (DCFLs).

To create DPDAs, we need to maintain the principle of determinism, at each step of computation, the DPDA has **at most** one way to proceed according to its transition function.

However, they are not as simple as DFAs, since we have two important things to consider:

Allowing $\epsilon$ moves:

1. read input without popping stack symbol
2. pop stack symbol without reading input

These moves are then the *two forms*, which are the $\epsilon$-input moves that look like $\delta(q,\epsilon,x)$, and the $\epsilon$-stack moves, which are $\delta(q,a,\epsilon)$.

- $\delta(q,\epsilon,x)$ corresponds to being in a state *q*, reading nothing from input, and pushing onto the stack
- $\delta(q,a,\epsilon)$ corresponds to being in a state *q*, reading a symbol from input, and doing nothing with the stack.

We can also combine both forms, as in $\delta(q,\epsilon,\epsilon)$. If a DPDA can make any empty move in a situation, then its not allowed to move a move in that same direction that involves using a symbol other than the empty symbol.

Formally, the DPDA is a 6-tuple, same with the PDA, however we add rules to the transition function:

- $\delta(q,a,x)$
- $\delta(q,a,\epsilon)$
- $\delta(q,\epsilon,x)$
- $\delta(q,\epsilon,\epsilon)$
    - For every possible state, input symbol, or stack symbol, EXACTLY one of these, is not a dead state.

The notation for transitions (shorthand) looks like: $\delta(q,a,x) = (r,y)$ which means the following things for the machine:

- If it is currently in state *q*, and the current input symbol is *a*, and the top of the stack has element *x*, then it goes to state r **while popping element *x*, and pushing *y* onto the stack.

Thus, the condition applied to the transition function, that one of the four possible forms of empty moves is non-empty means that for each state, input, and stack, there is always ONE possible move, and not zero, or more than one.

<aside>
💡 Lemma 2.41

Every DPDA has an *equivalent* DPDA that always reads the *entire* input string

</aside>

DPDAs are confusing, because there are DPDAs that *fail* to read the entire input string, because they try to *pop an empty stack* (hanging) or if it makes an *infinite* *sequence of empty-input moves* (looping)

Hanging can be solved by using the special stack character to symbolize an empty stack, thus, if the DPDA reads to the end of the stack, but not the end of the input, we know this must be rejection.

Looping is solved by understanding *when* the DPDA loops, such as times when the machine will read no more input symbols. In these situations, we must then add ways for the machine to read input and reject instead of infinitely looping.

We also need to also remember to always accept after the last symbol has been read, to avoid hanging or looping states which would lead to rejects.

---

[https://www.youtube.com/watch?v=GzR5FiiIogY](https://www.youtube.com/watch?v=GzR5FiiIogY) (Deterministic Context-free Languages)

- DPDAs have EXACTLY 1 transition to apply, for any given situation. Meaning that there is no ambiguity in the decision we make to transition, we can always only take 1 transition based on the input and the stack.
    - even though we may have transitions that rely on the same input symbol, it’s important that we distinguish those choices using differences in stack symbols.
        - for example, the transitions (a, empty → empty), (b, x → y), (b, y → x)
            - we have two transitions for *b* as a symbol, but there is no choice to be made because each relies on different stack symbols.
        - using empty pops or transitions makes the transitions more difficult, because empty moves can “match” to multiple transitions
            - like (a, empty → empty) and (a, x → y) cannot coexist because (x) and (empty) will cause a branch in transitions, which is non-deterministic)
- Something interesting about this restriction to the transition function, is that it implies that all regular languages are DCFLs in themselves.
- Also, all DFCLs are also CFLS, this is because DCFLs are more restrictive.

Re-reading:

- 

---

## Properties of Deterministic Context-Free Languages (DCFLS)

<aside>
💡 Theorem 2.42

The class of DCFLS is closed under **complementation**

</aside>

The core idea behind this theorem’s proof is that like in DFAs, we can swap the accept and non-accept states to achieve the complement of the language that is recognized by the machine. However, in DPDAs, there is something that limits us from doing this normally: we must make sure that the DPDA does not accept input by going into both accept and non-accept states after the end of the input. This is solved by modifying when the DPDA can enter acceptance, which is by only reading states that always read an input. Once we guarantee that the DPDA can enter accept states only when it is about to read the next symbol, then we can swap acceptance states and achieve the complement.

- additionally, DCFLs are NOT closed under concatenation, union, or intersection

---

[https://www.youtube.com/watch?v=f5d1hJQZTAs](https://www.youtube.com/watch?v=f5d1hJQZTAs) (Complementation Proof)

- DPDAs can have empty read transitions that puts the machine into an accept state, and if we were to just nominally flip the accept and non-accept states as in a DFA, then it is possible to have some cases of transitions that cause complementative acceptance, but the strings that should accept, instead take this empty transition, causing it to not accept when it should.
    - this is a direct showing that that string would simultaneously be in the language of the original, and in the complement of the language, which is impossible; since those two sets must be disjoint.
    - In order to construct the machine that allows us to easily complement, we need to first handle the previous **hanging** and **looping** problems
        - hanging is getting stuck on a state because all outgoing connections require popping from the stack, and if the stack is empty, it will never resolve
        - looping is when a machine has some circuit or loop that can take empty moves, and results in the machine never resolving because it is continuously changing states.
    - HANG: To solve these problems, we can use a special stack character ‘$’ to symbolize the end/empty stack, with no input characters inside of the stack.
        - We can create a new initial node that goes to the original initial node, via the transition $\epsilon, \epsilon \rarr \$$, which pushes $ onto the stack and always takes this transition because it requires no reads, nor pops.
    - HANG: Then, we wire all accept states to a NEW accept state, that transition using $\epsilon, \$ \rarr \epsilon$. Which essentially just means if we were to orignally accept, we must make sure that the new machine only accepts when the stack is empty and it were to accept.
        - This accept state will also use the transition $a, \epsilon \rarr \epsilon$ to the reject, which is a failsafe to make sure that we do not accept if we have an empty stack and are supposed to accept, but the input is not done.
    - HANG: In all intermediate (non-accept states) that are also not initial, we can also trap early stack situations by transitioning from all intermediate (non-accept) states to a ‘dead’ or ‘trap’ state that will reject, and will self-loop for all input symbols to keep it trapped.
    - LOOP: What about when we have a sequence of states that forms a cycle? This typically happens then the states in this sequence do not read, and the size of the stack does not get *smaller* or does not change.
        - We can have **accepting loops** if there is a state within the cycle that accepts, and a **rejecting loop** if there are no final states (accepting) in the entire cycle.
        - To solve this, we want to *redirect* each state in the cycle depending on whether or not the state accepts. For all the non-accept states in the cycle, we want to redirect each of them to the ‘dead’ state, popping the top symbol. For each accept-state then, we want to do the same for the opposite, so we redirect accept states to the new accept state popping the top symbol.
    - Using these new transitions, we guarantee that we will not loop, because when we are in the NEW accept state, if we have not read the whole string we reject. And we guarantee that we will not crash/deadlock by reading an empty stack, because if we see ‘$’ at any point within the intermediate states, it will automatically reject.
- It’s also useful to be able to breakup states that both read and pop into two states rather than just one.
    - If we had the transition from q → q’, with the rule (a,x → y) this means that when ‘a’ is on input, and ‘x’ is on top of stack, then we transition to q’, pop x, and push y.
    - If we break this up into two states, we then have q → q’’ → q’, for which we can have the pop on q, and the read on q’’.
        - q ($\epsilon, x \rarr \epsilon$) → q’’ ($a, \epsilon \rarr y$) → q’
        - this effectively does the same operation in the same order, because once ‘q’ is reached, then we automatically transition if ‘x’ is on top of stack while popping ‘x’, and arrive at q’’. Then if q’’ is current, and the active input is ‘a’, we pop nothing, and transition to q’ while pushing y.
            - q’’ is then called a **reading state** because it performs only a reading function, and not at all popping function.
            - importantly, if q (popping state) is an accept state, then q’’ (reading state) must also be an accept state, and vice versa.
- Also, if there exists a state in the DPDA that has transitions, none of which are *reading* transitions (it reads no input), that state is then not in the original set of final states (ie. if it exists in the final state set, remove it)
    - this guarantees the fact that if this non-reading state were an accept state, it could possibly exit this accept state without reading. This is not allowed, because it changes the language, so we need to not have this non-reading state in the set of accept states.
    - to get a little more complicated, this is allowed ONLY if we do not leave accept states for non-accept states WITHOUT reading.
        - we can have a chain of accept states that have no reading transitions and they are okay, but we are only allowed to leave an accept state into a non-accept state when we are reading a symbol from input.
- Once we hafve all these changes to the DPDA (if any were made), then we can finally swap the final and non-final states like a DFA, to achieve the *complemented machine* and complement language.
    - Without these changes, we may end up not capturing the actual complement language because there may exist cases for which strings are supposed to be in the complement, but aren’t and thus create impossible languages because they exist in the true complement, but are accepted in the non-complement, which is impossibly.

---

This theorem also helps us understand that **some CFLs are NOT DCFLs**, because any CFL whose complement is *not* a CFL, is not a DCFL.

DCFLs are also *not* closed under union, intersection, star, and reversal.

A handy notation we can adopt is the use of **endmarked inputs**, which a special marker is added to the end of the input string ($\dashv$), which is now adopted into the input alphabet. Endmarkers do not change the power of DPDAs, but is used to make designed DPDAs easier because we explicitly know when the input ends.

<aside>
💡 Theorem 2.43

A is a DCFL if and only if A$\dashv$ is a DFCL.

</aside>

(since this is IFF, we must prove both ways). To prove this forward, if some DPDA recognizes a language A, then P’ recognizes A$\dashv$ by simulating P until P’ reads $\dashv$. It is at that point which P’ accepts if P had entered an accept during the previous symbol, and P’ doesn’t read any symbols after the endmarker. In reverse, the DPDA P’ recognizes A, and as P’ reads input, it simulates P. Before any input reads, P’ determines if P would accept if that symbol were $\dashv$. If it does, then P’ enters an accept.

## Deterministic Context-Free Grammars

Both models of deterministic context-free grammars and pushdowns are equivalent in power as long as we use the endmarked languages.

This makes *deterministic* context-free methods not as strong as the bonds between regex+FAs, or even in normal CFGs+PDAs.

Determinism defines the path of deterministic choices, that is that we cannot make “choices” or “guesses”, and there is only a single possibility of choice in any single point or state. Determinism applied to grammars means that we have to modify the derivations and how we impart substitutions.

Normal (non-deterministic) derivations start with the start variable, and go **top-down** in substitutions according to the rules.

Deterministic CFGs take a **bottom-up** approach, by starting with terminal strings, and reversing the derivation to try and ‘reduce’ or ‘compress’ the string into variables. To do this, we use the *left-hand side* of a variable’s definition, called the *reducing string*. This entire process is called a **reduction**.

Deterministic CFGs are reductions that have certain properties.

Here is the notation of reductions:

<aside>
💡 u, v are strings of variables and terminals

$$
u \rightarrowtail v
$$

- *v* can be obtained from *u* by a reduction step
- $u \rightarrowtail v = v \implies u$.
- A *reduction from u to v* is a sequence of reductions from u to v, and that means *u* is **reducible** to *v*, so $u \overset{*}{\rightarrowtail}v$ represents that u reduces to v, and transversely $v \overset{*}{\implies}u$, v derives to u.
</aside>

We also have the idea of a standard order of reductions, called the **leftmost reduction**, where each reducing string is reduced ONLY after ALL OTHER reducing strings to its left are reduced.

Leftmost reduction = rightmost derivation, reversed.

If we have a string *w* that is in the language, a left most reduction looks like:

$$
w = u_1 \rightarrowtail u_2 \rightarrowtail...\rightarrowtail u_k = S
$$

Where we start with *w,* in the language, and reduce it to the start variable S defined in the grammar.

Determinism in grammars also requires that the leftmost reduction of any $u_i$ doesn’t depend alone on the symbols within $u_i$ to the right of the reducing string. Since this is so, the *w* string determines its entire leftmost reduction, and implies that the grammar must be unambiguous

To precisely capture reduction in the fullest, we define the substring that’s part of *w* that can be substituted for a variable (backwards derivation), the **handle** of a string $u_i$.

$$
u_i = xhy \rightarrowtail xTy
$$

Where the rule $T \rarr h$ exists, so we start with *xhy* and reduce the *handle* h so $u_{i+1} = xTy$.

In this case, *x* is the arbitrary left side of the handle, and *y* is the right side. 

Handles only exist in **valid strings,** which are strings that appear in leftmost *reductions* of some string in L(G), the language defined by the grammar. Valid strings can have many handles, but only when the grammar is *ambiguous*, because *unambiguous grammars* can generate strings with only ONE parse tree per derivation. Thus, unambiguous (deterministic) grammars have only a *single* handle

Think of this as the single building point of the string...

As an additional note, *y* must also be entirely terminals, because if it were to contain a variable, it would have been reduced earlier.

---

Example of Reduction

The language is the union between two languages **B** and **C**:

$B = \{a^mb^m\}$, $C = \{a^mb^{2m}\}$

Reduction of string: **aaabbb** which is in the language:

aaabbb → aaSbb → aSb → S → R

- the handle of this string is underlined.

Reduction of string: **aaabbbbbb**

aaabbbbbb → aaTbbbb → aTbb → T → R

![Untitled](repo/wlu/computerScience/CP414/CP414%20Foundations%20of%20Computing/2%20=%20Context-Free%20Languages%20f2d3a11242664e02afc23ed045104bbb/Untitled%2017.png)

This language also must be recognized by a (non-deterministic) pushdown, because it needs determinism to *guess* whether or not the input is in B OR C, because after it pushes all the *a*’s onto the stack, it can split compute how to match a’s to b’s: either (ab) or (abb). A DPDA cannot know whether the input is in B or C, so it cannot guess because it is deterministic.

---

However, what if the grammar provided us with some *encoded information* as to which language a string may be apart of, as in the grammatic rules:

$$
\begin{cases}R\rarr1S|2T\\S\rarr aSb|ab\\ T\rarr aTbb|abb\end{cases}
$$

Where the ‘1’ or the ‘2’ can indicate the language. Thus, to define DCFGs, we need to include languages like this one, but exclude languages that are the previous ones.

---

Examples of Endmarked Reductions

$$
G_3 = \begin{cases}S\rarr T\dashv\\T\rarr T(T)|\epsilon\end{cases}
$$

In this grammar, we have *endmarkers* and *empty* handles. We can see how this works by reducing the string “()()$\dashv$”

 ()()$\dashv$ → T( )()$\dashv$ → T(T)()$\dashv$ → T( )$\dashv$ → T(T)$\dashv$ → T$\dashv$ → S

- the empty handle allows us to replace empty spaces in the string with “T”, which can build the reduction string T(T) that we can eventually reduce to T itself.

---

The entire point of defining grammars this way is to make them equivalent to DPDAs so that they can define the same classes of languages, because of the deterministic, restrictive natures. Because of this, DPDAs need to find *handles* so then it can find reductions. But this is a sort of ‘lookahead’, because to define the correct handle, we need to know the reduction, which is not possible in a DPDA. Thus, we need to restrict handles even more so DPDAs can easily find them.

The reasoning behind this is that, while we have restricted *deterministic* context-free grammars to be unambiguous so that it has a single handle, this is not enough. This is because we still may need to lookahead in the string to figure out what the handle is. So, the restriction that we impose on handles to make them easily identifiable to DPDAs is that the *initial* part of a valid string, up to and include the handle itself, must be sufficient to determine the handle.

- this means that the handle ‘ab’ must be significantly different from the handle ‘abb’, because in order to identify whether it is ‘ab’ in some case, then we need to lookahead of the first ‘b’ to find a second ‘b’ (for each ‘a’). Thus, it disqualifies handles like this, so we need to make handles that are different enough such that reading the initial part of the valid string can tell us exactly which handle it is alone, without needing to compare to the other handle.
    - a sort of reasoning that looks like “this substring has a 1, thus it must be the handle”.

A handle *h* of the valid string v = *xhy* is a **forced handle** if *h* is the unique handle in *every valid string* $xh\hat{y}$, in which $\hat{y} \in \Sigma^*$.

<aside>
💡 **Deterministic Context-Free Grammar**

a context-free grammar such that every valid string, has a *forced handle*.

</aside>

- Also to simplify, the start variables of  grammar does not appear on the right-hand side of any rule
- and every variable appears in a reduction of some string in the language, meaning there are no useless variables.

In order to determine whether any CFG is deterministic, we can use the **DK-test**, and is also a useful way to enable DPDAs to find handles.

<aside>
💡 DK-test

Relies on the fact that for any CFG G, there is a way to construct the associated DFA called DK that can identify handles. Thus, the DFA DK accepts the input *z* if:

1. *z* is the **prefix** of some valid string, such as v = *zy*.
2. *z* ends with a handle of *v*.
</aside>

- Each accepting state of the DFA called DK, is a state that is associated with the reducing rules. Because in a DCFG, each accept state corresponds to *exactly one* reducing rule.

Since all handles in a DCFG are *forced*, then *zy* is a valid string that has a prefix *z,* which ends in a handle of *zy;* which is unique. Thus, each of DK’s accept states associate with a single handle, and with a single reducing rule. Also, the accept states must not have outgoing paths to *other* accept states, otherwise, it would mean that it could accept by reading some string in *y*, which is any other string, which means that *zy* would not be unique, or depends on *y*. This is a way to construct the DFA DK, and that G is deterministic, if all accept states have the properties.

We can construct DK by using an equivalent NFA ‘K’, for which K → DK.

This NFA K is a simple NFA, which accepts *every input* string that *ends with* the right-hand side of any rule in the grammar. This is simple because it can *guess* each rule, and guesses where to start matching input.

We can track the progress of the chosen right-hand side by using a **dot** in the point at which we are at in the rule, giving us a **dotted rule** (or **item**). For each rule with *k* symbols on the RHS, there are then (k+1) dotted rules.

- Each dotted rule is *1* state of J (the simpler version of K).
- The state associated with a *dotted rule* B → u.v, is notated using a *box* around it, with the *accept states* B → u. corresponding to completed rules, and notated by a double-border box.

In order to also construct K from the simpler J NFA, the difference is that K is more careful about matching rules. Along with J, K also has states that correspond to all dotted rules, but has a special start state that has an empty-move to the state $S_1 \rarr .u$, for every rule that involves the start variable S1.

For each branch of computation, K matches a potential reducing rules, with a substring of input. Any right-hand sides of a rule that contain a variable allow K to non-deterministically branch to a rule that expands that variable.

Transitions of K are two-types:

1. shift moves
    1. for every *a* that is a terminal or variable, and every rule B → *uav*
        1. $B\rarr u.av \overset{a}{\rarr}B\rarr ua.v$
2. $\epsilon$ moves
    1. for all rules B → *uCv* and C → r
        1. $B \rarr u.Cv \overset{\epsilon}{\rarr}C\rarr .r$

K will accept all strings *z* that end with handles, and because K is nondeterministic, then it ‘may’ enter a state, meaning that it can possibly branch to that state via non-determinism.

<aside>
💡 Lemma 2.48

K may enter a state [T → u.v] on reading input *z*, if and ONLY if *z = xu*, and *xuvy* is a valid string, with the handle *uv* and the reducing rule T → uv, for some $y \in \Sigma^*$

</aside>

<aside>
💡 Corollary 2.51

K may enter an accept state [[T → h.]] on an input *z* if and only if *z = xh*, and *h* is a handle of some valid string *xhy* with reducing rule T → h.

</aside>

Once we have the NFA ‘K’, we then construct DK by using the subset construction from NFA → DFA. Thus, each of DK’s states have ≥ 1 dotted rules, while each accept states have ≥ 1 completed rule.

Once we then have DK, we can determine if a CFG ‘G’ is deterministic by understanding DK’s accept states.

<aside>
💡 Theorem 2.52

a CFG ‘G’ passes the DK-test if and ONLY if G is a DCFG.

</aside>

---

DCFGs Revisited

- So far, it seems as though “handles” define the RHS of the grammar rule, and the reducing string is the LHS?
    - this is probably wrong lmao
- We use reduction in order to uniquely define DCFGs and constructing a concise model for why they are deterministic
    - thus, if we use a backwards derivation, starting from the derived string of terminals, it’s much easier to see that in order to make a context-free grammar *deterministic,* we need to identify the non-deterministic parts using reduction.
    - when we start with a string derived from a grammar, and reduce it to the starting variable, we can see that within the string of terminals, there are three important sections to this string: xhz.
        - *h* is the most important, because it is the *handle* that we use to reduce the string. A grammar has several handles in this view, but as long as the grammar is **unambiguous**, there should be no trouble choosing a handle to use to reduce the string.
        - If a grammar is ambiguous, that means at some points in the reduction, we might have multiple choices to choose from to achieve the starting variable; this is non-deterministic, and thus we cannot have ambiguous grammars being deterministic because of this choice.
        - so, if a grammar is unambiguous, that means at any point in the reduction, we’ll have a direct path to the starting variable without choosing or guessing, so each handle is then **unique**.
    - because we’ve reasoned that unambiguous grammars have unique handles, does not mean that we have achieved determinism. Because there then exists unique handles which are different, but essentially not different enough.
        - take ‘ab’ and ‘abb’ as two handles, they are precisely unique, but are similar. Thus, when we reduce a string, and we have the string form [x’ab’y], where x and y are strings of terminals, we cannot know for sure which one it is
            - because as soon as we read ‘ab’, in order to confirm it as precisely the ‘ab’ handle, we need to continue reading ‘ab..’ to see which handle to use.
            - this “lookahead” is something that DPDAs CANNOT do, so since we are trying to reason that DCFGs are equivalent to DPDAs and construct them, this is not possible for DCFGs.
    - Along with uniqueness which is guaranteed from the fact that the grammar must be unambiguous, we need to assert that in order to DCFGs to equal DPDAs, then we introduce **forced handles**.
        - forced handles are defined as handles which are unique handles in **every** valid string (xhy), where y is in the set of all strings.
        - this means that between any two valid strings (which are the intermediate strings produced in reductions), this handle must be totally unique in every occurrence.
        - forced handles are a way to guarantee that for all characters up to, and including the handle, allow us to identify the handle without looking into the *y* portion.
    - Thus, DCFGs are defined as CFGs which has forced handles for every *valid* string.
- The DK-test (deterministic K) is a test that we use to confirm whether any CFG is deterministic
    - it relies entirely on the fact that the language of handles for a grammar itself are regular, so we can build a DFA called (DK) to identify handles of the CFG.
    - DK will accept the input if the input is a **prefix** of some valid string, and the input ends in a handle of v.
        - this means that in the 3-section format, the input is the form: xh.
    - The accept states of DK will also be associated to the reducing rules, because for any DCFG (since it is deterministic), there is exactly 1 reducing rule associated with a handle.
    - It’s also called Deterministic K, because to easily construct the handle identification, we can construct a simple NFA using the handles and the rules, and then make it deterministic (NFA → DFA)
        - The easy way to construct K, is to construct a simpler NFA, J, which just accepts any input which ENDS in the RHS of any reducing rule (the handles)
            - it does this by guessing which rule to match, and at what point to guess

## Relationship between DPDAs and DCFGs

- DPDAs and DCFGs describe the *same class* of **endmarked** languages, but we must understand also how to convert DCFGs into their equivalent DPDAs, and vice versa.
- Endmarkers affect the class of languages that are *generated* by DCFGs, but not recognized by DPDAs.
    - Without endmarkers, DCFGs generate only subclasses of DCFLs, which are prefix-free

<aside>
💡 Theorem 2.57

Any endmarked language is generated by a DCFG if and only if it is itself *deterministic context-free.*

</aside>

Thus, the proof of this implicitly shows that every DCFG has an equivalent DPDA, and every DPDA that recog. some endmarked language, has an equivalent DCFG.

<aside>
💡 Lemma 2.58

Every DCFG has an equivalent DPDA.

</aside>

The idea of this proof uses the DFA DK as a simulation through the equivalent DPDA called P.

<aside>
💡 Lemma 2.59

Every DPDA that recognizes an endmarked lagnuage, has an equivalent DCFG.

</aside>

## Parsing and LR(k) Grammars

DCFLs are practically useful, because algorithms of membership and parsing that are based on any DPDAs are efficient. Sometimes however, DCFGs are very limiting for expressing particular DCFLs, because of the limit that all handles must be forced.

There exists a broader class of grammars, the LR(*k*) grammars that are a combination of both classes, because they are close enough to DCFGs to be equivalent to DPDAs, and are more expressive of certain languages.

- This is because they use **lookahead** algorithms. Forced handles require the substring before and including the handle to be unique, whereas lookahead algorithms we can use the terminal symbols *after* the handle - but only the first **k** terminals.
- LR(k) stands for “Left-to-right input processing, Rightmost derivations (leftmost reductions), k symbols of lookahead”.

If *h* is a handle of a valid string, *v = xhy*, and *h* is **forced by lookahead k**, that means that *h* is the unique handle of *every valid string* $xh\hat{y}, y\in \Sigma^*$, where y and $\hat{y}$ agree on the *first k symbols*.