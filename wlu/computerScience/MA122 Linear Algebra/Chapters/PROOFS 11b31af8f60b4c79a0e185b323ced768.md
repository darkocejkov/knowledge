# PROOFS

This page is meant to break down the various proofs shown in the practice problems of the textbook.

# Intro

Proofs can be daunting if you've never done them before, as I know some people are taking this course (MA122) before MA121, it'll be a good practice for the hell that is proofs. These proofs in MA122 are actually mid-level proofs, they require knowing and understanding about the details of the concept that you're trying to prove. Proofs are quite difficult if you don't understand the links between the concepts here, so I definitely suggest getting a strong foundation of the basics of linear algebra.

Even if you don't know how to prove something, there's still marks in attempting a proof. The important parts are the different sections of a proof. Let's break down the wording of a proof:

<aside>
üí° "If X, then Y"

This is the most used logic in any proof, an "if-then" statement that proposes that if you know that X is true, prove that Y is true. In terms of logical notation, then this is represented as $P \rarr Q$ , as in "If P, then Q". The point here is that the *P* is the assertion, or the statement that you know is true. Then, we have to use P to prove that Q is also true. For example, you'd see this as something like "If a matrix A has a NON-ZERO determinant, then it is invertible". So, we'd have to hinge on the fact that det(A) ‚â† 0, and use that to prove that it is, in fact, invertible BECAUSE of the determinant being non-zero.

</aside>

Even knowing this, there are many different ways to approach proving this statement. We can use direct definitions of invertability, like the fact that all invertible matrices have non-zero determinants. Or, we can try and prove it by proving the opposite is always false. For example, try finding a matrix that is invertible, with a determinant of 0. If we can prove that this is not true in the abstract, general case, then automatically we've proven that a matrix with non-zero determinant is invertible. This is called a **proof by contradiction**, when we assert the opposite of the statement, and find a contradiction like "1 = 0", something that is just false.

In the textbook, you also see different assertions of "if P then Q", and it's most often in the form "P, if and ONLY if Q". The wording is slightly-misleading, because what this *really* means is that this is true *both ways*. If we know that P is true, Q is true; if we know that Q is true, then P is true. This is most often proven from "both sides", by assuming only P or Q, one-at-a-time. So this is like saying:

<aside>
üí° "If P, then Q AND if Q, then P".

To prove this, you can use any form of proving P or Q, but you usually do it both ways to solidify that it really is both ways. So you'd first do "if P, ... arguments..., then Q", then "if Q, ..., arguments..., then P".

</aside>

One thing to note is that proofs are *much* easier when you are given a direction to go. Proving something is ONLY true or ONLY false, means that you are hinted towards that it is one, and not the other. So 

Also, difficult proofs might rely heavily on a "trick" to move the proof along. If you've ever proved something, you might get far, but are completely stopped because you don't know how to continue. These points are the tricky points, because they might need you to do some really weird algebra to get the statement in a more advantageous position.

A much more difficult kind of proof is one through **induction**, which you'll need to do in MA121, but *probably* not this course. If the exam for MA122 includes a proof by induction, then it's pretty clear that the professor has no concern for your well-being, or their teaching degree for that matter, because MA121 is NOT a requirement for this course.

Induction involves proving explicitly by steps, and proving that the **base case** is always true. For example, if 1 + 1 = 2, then prove how a + b = c. This can be done through induction because we know the base case (0 + 0 = 0) or (0 + 1 = 1) is true. The main point of induction, is that we use the base case to form the induction, which is generalizing the base case. So we can say that $0 + a = a$, by identity. This induction will be true for any $a \le 1$. So, to prove that for any $a, b \in \Z, a+b = c$, we need to prove that for the identity $0+a=a$ is true for any $a > 1$.

## Truths

When you try proving a statement, try and think critically about why something is proven, or how your arguments truly prove what you are trying to. In a proof, you use different things that you know to be true, otherwise, you're not really proving something. You can't really prove the sky is blue, using the zodiac signs. That sounds silly, because it is very false, we cannot *verify* that the zodiac signs have any truths at all, or how it is related to the sky being blue, right? So, these *truths* are usually called *axioms*, *lemmas*, and *corollaries*. These are like "small truths" that would be very "common sense", but are used in proofs because they are verifiably true, and we can use them as arguments.

For example, here's a lemma: $1+1 = 2$. This is true because by definition of adding, 1+1 does in fact equal 2. Now, how do we know that 1+1+1= 3? Because (1+1) = 2, so 2+1 = 3, by defintion of adding.

The point here is that if we know that something is true, we can use it as a "stepping stone" to modify the previous arguments, and reach closer to the end goal.

## Methodology

The easiest way to think about a proof is as a puzzle. You have a goal in a puzzle, and that's to solve it. What is true between any puzzle, jigsaw or not, is that you always have some pieces of the puzzle. The idea is to utilize all your pieces that you have, which is usually given by the statement of the proof. The first IF statement is a big hint, because that should usually be our first piece that we put down. After that, we need to think about how the different rules and properties of linear algebra fit into this statement, and how certain pieces can allow for more arguments to be made.

To start off a proof, always think about what is really being asked, and start from there. Layout the proof in such a way that it is 100% clear to you, what you are trying to prove.

Personally, I've had to prove some really tough statements before, and it doesn't always click why a certain proof makes sense. I

## Example Proof

Think about what it means for a number to be *odd* or *even*. Think about the *form* of an odd or even number. An even number always has the form $2a=b$. Where $a,b\in \Z$, and any even number *b* is expressed as a scalar multiple of 2. That's what it means to be even, that $b/2=a$. What about odd numbers? Well, think about them like $2a+1=b$, where b is an odd number. So, if $a, b \in \Z$, then prove that all sums $odd+odd = even$.

This prove is by direct verification using the definitions of even and odd *integers*. 

Since $odd = 2a+1$, then $odd + odd = (2a+1)+(2b+1)$, where 2a+1 and 2b+1 are two different odd numbers. By addition, $2a+2b+2=2(a+b)+2$. So ,we can see that, thru factoring out the 2 in the coefficients of *a* and *b*, then we get some new number. Now, since we know that $a,b \in \Z$, then if we add two integers together, then $a+b = c$, where *c* is also an integer. So, $2c+2$. So, is there a difference between $2c$ and $2c+2$, in terms of even or odd numbers? No, because we know that all integers will alternate even, odd, even, ..., etc. So, because we have the form $2c+2$, we know that $2c$ is what makes an integer even, and adding two, makes it go from even, to odd, back to even. So, the sum of two odd numbers is even $\blacksquare$.

Also, we could've gone a slightly different way, instead of $2(a+b)+2$, it could be $2(a+b+1)$. Then we'd need to use the fact that $a+b+1$ is also an integer, and that $2(a+b+1)$ is even because $even \times odd = even$, by the definition of $2a = b$.

(The black square denotes that a proof has ended).

## In terms of Linear Algebra

So now that we understand the form of a proof, where we use the statement to step towards the end goal, the THEN statement, let's think about how to use this in Linear Algebra. Over the entire course we're always learning about the *properties* of different concepts, like invertible matrices, eigenvalues and such, square matrices, special matrices, etc. A lot of the proofs that you'll see use these properties to make arguments with. So it is very beneficial to have some sheet that has all the properties of the various concepts. More importantly because you know what you are trying to prove (the THEN statement), this gives you a huge boost in choosing your arguments. If we are proving how with invertible matrices A and B, that A is similar to B, you would narrow down your properties to deal with invertibility and matrix products.

Proofs are also never just 1 answer, there are so many different ways to prove different things, as long as your arguments work together and you use each ladder rung below you, to move to the next. You can't skip steps, otherwise, how do we know that the step you skipped is relevant, or true?

# * These proofs are done by myself, and are verified by the textbook/professor solutions. There are many ways to do each proof, the main point of this is to see a proof with explanation to understand how to string together arguments.

# Chapter 4

<aside>
‚≠ê **If A is invertible, and $n > 1$, then $det(adj(A)) = (det(A))^{n-1}$.**

By the definition of invertability, we know that $A*adj(A) = det(A)I_n$. This is because the adjoint has all the *cofactors* of A transposed, and if we multiply the adjoint of A with A itself, we get all the values of the form $a_1C_{11}$, which are the kinds of  values that we see in the cofactor expansion. Because the adjoint is transposed, the matrix $Aadj(A)$ is essentially, symmetrical in terms of the determinant, and all the non-diagonal entries cancel each-other out, while the diagonals are the determinant of A.

Then if we take the determinant of this matrix, $det(Aadj(A))$, then $det(det(A)I_n)$. Since det(A) is a scalar value (a real number), it's the same as saying $det(rA) = r^ndet(A)$, so:

$$
det(det(A)I_n)=det(A)^ndet(I_n)
$$

Since $det(I_n) = 1$, because it is diagonal and each diagonal entry is 1, then 1*1*...*1 = 1. We can say that $det(Aadj(A)) = det(A)^n$

Let's go back to $det(Aadj(A))$, and apply the product rule $det(AB) = det(A)det(B)$, so $det(Aadj(A)) = det(A)det(adj(A))$.

Since we know this determinant can expand, and at the same time is equal to $det(A)^n$, let's relate them:

$$
det(A)det(adj(A)) = det(A)^n
$$

Then, because determinants are real numbers, we can use typical algebra to find $det(adj(A))$ by multiplying both sides by $\frac{1}{det(A)}$:

$$
\frac{det(A)det(adj(A))}{det(A)}=\frac{det(A)^n}{det(A)}\\
det(adj(A)) = \frac{det(A)^n}{det(A)}
$$

Based on exponent rules, we know that $\frac{A^x}{A^y} = A^{x-y}$, so we can further simplify such that:

$$
det(adj(A)) = det(A)^{n-1}
$$

because $det(A)$ implicitly means $det(A)^1$. It also shows the fact that this ONLY WORKS if A is INVERTIBLE. This is because in the previous equation, we divide by the determinant of A, so if A is NOT INVERTIBLE, det(A) = 0, which makes this impossible. 

So, we've proven that if A is invertible, meaning that it has a non-zero determinant, the determinant of the adjoint matrix of A is equal to the $n-1$'th power of the determinant of A     $\blacksquare$

</aside>

<aside>
‚≠ê **Prove that if A of order is invertible, and n > 1, then adj(A) is ALSO invertible, and $[adj(A)]^{-1}=adj(A^{-1})$**

We know from the last proof, that $Aadj(A) = det(A)I_n$. This means that A multiplied by its adjoint matrix yields a matrix whose diagonal entries are all the determinant of A. We also know that since A is invertible in this stipulation, that $A^{-1}$ exists, and $det(A) \ne 0$. Because the det(A) is not 0, then the matrix of $det(A)I_n$ is the product of diagonal entries, which are the determinant, which is non-zero, and thus is also not 0. So the matrix $det(A)I_n$ is also invertible. So, because $Aadj(A) = det(A)I_n$, this means that $Aadj(A)$ is also invertible. Because this $Aadj(A)$ matrix is a matrix product, and it is invertible, then we know that both A and adj(A) are invertible because a matrix that is invertible must be the product of invertible matrices.

So, we can move further and say that $[adj(A)]^{-1}=\frac{1}{det(A)}A$. This is because $[Aadj(A)]^{-1}=adj(A)^{-1}A^{-1}$, which means that $adj(A)^{-1}=\frac{1}{det(adj(A))}A=\frac{1}{det(A)^{n-1}}A$.

Since we've just constructed this above *identity*, we can then take the adjoint of the inverse of A, like so:

$$
A^{-1}adj(A^{-1})\\
=det(A^{-1})I_n\\
=\frac{1}{det(A)}I_n
$$

Then, we can see that the two terms $adj(A)^{-1}$ and $adj(A^{-1})$ are slightly related by $\frac{1}{det(A)}$:

$$
A(A^{-1}adj(A^{-1}))=A\frac{1}{det(A)}I_n
$$

$$
adj(A^{-1})=\frac{1}{det(A)}A
$$

‚Üí This solution is actually very poorly solutioned in the official solutions for this course. Here's a better proof:

$$
A^{-1} = \frac{1}{det(A)}adj(A)\\
(A^{-1})^{-1}= \frac{1}{det(A^{-1})}adj(A^{-1})\\
A=\frac{1}{det(A^{-1})}adj(A^{-1})\\
det(A^{-1})A=adj(A^{-1})\\
\frac{1}{det(A)}A = adj(A^{-1})

$$

</aside>

<aside>
‚≠ê **Let A, B be invertible matrices of order n > 1. Prove that $adj(AB)=adj(B)adj(A)$**

Since we know that A and B are BOTH invertible, then we know that both AB and BA are also invertible by definition. We know that through the previous proofs, given that A/B are each invertible:

$$
adj(A) = det(A)A^{-1}\\
adj(B) = det(B)B^{-1}
$$

Then, it's not difficult to move forward knowing that AB and BA are also invertible:

$$
adj(AB)=det(AB)AB^{-1}
$$

Through the rules of determinants and products: $det(AB) = det(A)det(B)$:

$$
adj(AB)=det(A)det(B)AB^{-1}
$$

And since AB is invertible, its inverse is: $AB^{-1}=B^{-1}A^{-1}$:

$$
adj(AB)=det(A)det(B)B^{-1}A^{-1}\\
adj(AB)=det(A)A^{-1}det(B)B^{-1}\\adj(AB) = det(B)B^{-1}det(A)A^{-1}\\
adj(AB)=adj(B)adj(A)
$$

</aside>

# Chapter 5

<aside>
‚≠ê **Let A be a m x n-matrix, such that $A^TA$ is invertible. Prove that the columns of A are linearly independent.**

Because $A^TA$ is invertible, we know that both $A^T$ and $A$ are invertible. The definition of linear independence means that the *columns* of A have a *trivial, unique* solution to the homogenous system $Ax = 0$, and in this case, would look like: $(A^TA)x=0$.

The inverse of $A^TA = (A^TA)^{-1}=A^{-1}(A^T)^{-1}$. Because this matrix $A^TA$ is invertible, then there's a unique solution to the system $(A^TA)x=0$, which is:

$$
(A^TA)^{-1}(A^TA)x=0
$$

This simplifies since $(A^TA)^{-1}(A^TA) = I_n$ by the definition of invertibility, so $(A^TA)^{-1}(A^TA)x = 0 \rarr I_nx = 0 \rarr x = 0$. So, this is the unique solution to the homogenous system, and by the definition of linear independence, makes A linearly independent, as well as $A^T$.

</aside>

<aside>
‚≠ê **Let u, v, and w be three linearly dependent vectors in ~n. Suppose that v and w are linearly independent. Let (a, b, c) be any nontrivial solution to the linear system below with variables x, y, z such that: $xu+yv+zw = 0$; show that $a \ne 0$.**

We know that *v* and *w* are linearly independent, so there is no solution to $v = cw$ or $w = cv$, that is, they are not proportional nor equal to eachother. This means that by transversive properties, that $yv\ne zw$, or $zw \ne v$ or $yv \ne w$. This means that (in a linear combination), that there are no variables $y , z \ne 0$ such that $yv+zw = 0$. Then, we can re-arrange the initial equation to become:

$$
xu+yv+zw = 0\\
xy = -yv-zw
$$

Let's assume that  $a =0$, then we can see that $(0,y,z) \rarr 0(u)=-yv-zw \rarr 0 = -yv-zw$. This would directly mean that $yv = -zw$ or $zw = -yv$, and would contradict the idea that *v* and *w* are NOT linearly independent, and thus $a \ne 0$, in order to maintain the relationship between *v* and *w*.

</aside>

# Chapter 6

<aside>
‚≠ê Let $T: \R^n\rarr \R^m$ be a matrix transformation, and $v_1, v_2, ..., v_k$ is a collection of vectors in $\R^n$, and $a_1,a_2,...,a_k$ be a collection of *real* numbers. Prove:

$$
T(a_1v_1+a_2v_2+...+a_kv_k)=a_1T(v_1)+a_2T(v_2)+...+a_kT(v_k)
$$

Let's take the standard matrix of the transformation $[T] = A$. Applying a transformation is identical to multiplying a matrix, so we can re-write the Left hand side (LHS) to be:

$$
A(a_1v_1+a_2v_2+...+a_kv_k)
$$

Since each $a_iv_i$ is a vector, and matrix product *distributes* on the left here, we can see that:

$$
Aa_1v_1+...+Aa_kv_k
$$

We know by the properties of matrix product, $Ac = cA$ means that scalar multiplication commutes, leaving $a_1Av_1+...+a_kAv_k$. This would mean that in terms of transformation notation now, $a_1T(v_1)+...+a_kT(v_k)$.

</aside>

<aside>
‚≠ê **Given $T: \R^n \rarr \R^m$ being a matrix transformation, then:**

a) **prove that if *v* and *w* (vectors) are linearly DEPENDENT, then T(v) and T(w) are ALSO linearly dependent.**

Since we know that *v* and *w* are linearly dependent, then $v=cw$ or $w = cw$ is true, meaning that they are proportional. In other words, this means that there is a non-trivial solution to the equation: $av + bw= 0$.  Since we are dealing with a transformation, then $aT(v)+bT(w) = T(av+bw)=T(0)= 0$. This means that T(v) and T(w) is also linearly dependent, because $a,b \ne 0$, and they are related by a linear combination, and they equate to 0, meaning that there are $a,b \ne 0$ that make this equation also true, thus they are linearly dependent.

b) **The converse of a) is false, that if T(v) and T(w) are linearly dependent, then *v* and *w* are linearly dependent.**

We can prove this false easily by providing a *case* of *v* and *w* that are linearly independent, but T(v) and T(w) are linearly dependent. Take these two vectors $v = [1,0], w=[0,1]$. The transformation $T(x,y) = (x+y,x+y)$ when applied to *v* and *w*, are these:

$$
T(v) = (1+0, 1+0) = [1,1]\\
T(w) = (0+1,0+1) = [1,1]
$$

       Thus, we can clearly see that V and W are NOT linearly dependent, while T(v) and T(w) are, so this converse statement is FALSE.

c) **For vectors *v* and *w*, if T(v) and T(w) are linearly indepedent, then v *and w* must be linearly independent.**

This means that $T(v) \ne bT(w)$, and that the equation $aT(v) +bT(w) = 0$ has no non-trivial solution. Because of linearity properties of transformations, we can see that:

$$
aT(v)+bT(w)=T(av+bw)=0
$$

With the same variables $a,b=0$ being the only solution to the equation, it's also the same here, when applied to the vectors *v* and *w* themselves, and we can see that there would be no non-trivial solution for that linear combination, so v and w must be linearly independent.

</aside>

# Chapter 7

<aside>
‚≠ê **A, B are squares of order *n*, and $v \in \R^n$ is an eigenvector of BOTH A and B, prove that vector *v* is an eigenvector of $AB$ and $BA$.**

Since we know that (by definition of eigenvectors), $Av = \lambda_1v$ and $Bv = \lambda_2v$, then for $AB$ we have that $(Av)(Bv) = (\lambda_1v)(\lambda_2v) = (\lambda_1\lambda_2)v$, so since $\lambda_1\lambda_2$ is an eigenvalue of $AB$, we know that $\lambda_2\lambda_1$ is also the same eigenvalue, since multiplication of $\R$ is commutative ($ab = ba)$. So, that leads us to show that $(AB)v = (\lambda_1\lambda_2)v$ is the same as $(BA)v = (\lambda_2\lambda_1)v$        $\blacksquare$

</aside>

<aside>
‚≠ê **Given that matrices A and B are squares of order *n*, if A is invertible, then $AB$ and $BA$ have the *same characteristic polynomial*.**

Recalling back to what a characteristic polynomial is, it is the determinant of $(\lambda I_n - A)$, whose *roots* $(\lambda+x)(\lambda+y)$ (for example), are the *eigenvalues* of the matrix A. To prove that $AB$ and $BA$ have the same characteristic polynomial, we need to determine the *arbitrary* characteristic polynomials for $AB$ and $BA$, and relate them.

When a characteristic polynomial isn't factored, you can see that $det(\lambda I_n - A) = \lambda^n + c_1\lambda^{n-1}+...+c_n$.

However, we don't need the expanded portion, because it would be to difficult to prove that using arbitrary units. So, we need to prove that $det(\lambda I_n - AB) = det(\lambda I_n -BA)$. The first step here is to understand that A is *invertible*, meaning that $A^{-1}$ exists, and that $AA^{-1}= I_n$.

$$
=det(\lambda I_n -AB)\\
=det(\lambda(AA^{-1}) -(AB)I_n)\\
=det(\lambda AA^{-1}-ABAA^{-1})\\
=det(A^{-1}(\lambda I_n - BA)A)\\
=det(A^{-1})det(\lambda I_n - BA)det(A)\\
det(A^{-1}) = \frac{1}{det(A)}\\
=\frac{det(A)}{det(A)}det(\lambda I_n - BA)\\
= det(\lambda I_n - BA)
$$

So we start off with defining the "characteristic polynomial of AB" using the definition involving the determinant. Then, since A is invertible, and $AA^{-1} = I_n$, we can "replace" $I_n$ in $\lambda I_n$, and also multiply $(AB)I_n$, so that the equation within the determinant is still valid.  We can then *expand* the $I_n$ to involve A and its inverse. Now, this is the weird part of the proof, we need to *group* the terms like we are factoring or cancelling out the extra terms. Remember, that in order to *distribute* matrices, we need to pay attention to the SIDE that they are on, since $A(B+C) = AB + BC$ is not the same as $(B+C)A = BA+CA$, since $AB\ne BA$. Once we factor the inverse of A and A itself, we can use the property of determinants such that $det(AB) = det(A)det(B)$, and apply that to the determinant. Then we get three different determinants, the determinant of A, determinant of inverse A, and determinant of char. poly. of AB. Since we know that the determinant of the inverse of A is 1/det(A), then we can simply this equation so we get det(A)/det(A), which is always 1. So, we've proven that the characteristic poly. of AB is the same as BA, given that A is invertible;      $\blacksquare$

</aside>

<aside>
‚≠ê **Given A and B are both square matrices of the same order, and that they are diagonalized by the same matrix P, prove that $AB = BA$.**

We know that $P^{-1}AP = D$ and $P^{-1}BP = C$. By multiplying both equations on the LEFT with P, we get two equations:

$$
PP^{-1}AP = PD \rarr AP = PD\\
PP^{-1}BP = PC \rarr BP = PC
$$

And, if we multiply $P^{-1}$ to the RIGHT, we get:

$$
A = PDP^{-1}\\
B = PCP^{-1}
$$

So, if we relate AB the same way:

$$
P(AB)P^{-1} = PDCP^{-1} = AB
$$

$$
AB  = (PDP^{-1})(PCP^{-1})\\
AB = PD(P^{-1}P)CP^{-1}\\
AB = PDCP^{-1} = BA
$$

</aside>

<aside>
‚≠ê **Let A and B be squares of the same order, and that A is SIMILAR to B. Prove:**

**a) A and B have the SAME characteristic polynomial**

If A is similar to B, that means that $A = P^{-1}BP$, OR $B = P^{-1}AP$. So, in order to prove they have the same polynomial, we need to define them. We know the characteristic polynomial of A is: $det(\lambda I_n - A)$. Because A is similar to B, then we can replace A with the similar matrix B produced with P:

$$
det(\lambda I - P^{-1}BP)\\

$$

We then multiply the left of the inner relation by P, and the right by the inverse of P, to yield:

$$
det(P(\lambda I -B)P^{-1})\\
det(P)det(\lambda I - B)det(P^{-1})
$$

And since we know that $det(P^{-1}) = \frac{1}{det(P)}$, then $det(P)det(P^{-1}) = 1$, so this leaves the characteristic polynomial of B, meaning that A and B have the same characteristic polynomial.

**b) A is diagonalizable IF AND ONLY IF B is diagonalizable**

Since in this case B is diagonalizable, we have $Q^{-1}BQ$ is diagonal, and A is similar to B, meaning $A = P^{-1}BP$.

If we change this similarity to be in terms of B, then $B = PAP^{-1}$. This means that in $Q^{-1}BQ$, we can substitute B now, which becomes:

$$
Q^{-1}(PAP^{-1})Q
$$

Which means that the matrix $PAP^{-1}$ is diagonal, and thus A is diagonalizable.

</aside>

<aside>
‚≠ê **Suppose that a square matrix A is diagonalizable**

**a) Prove $A^T$ is also diagonalizable**

Since A is diagonalizable, then $P^{-1}AP = D$, where $D = diag(a_1, a_2, ..., a_n)$. Then, the matrix D has a transpose which is identical, because D is diagonal.

So, we know that $P^{-1}AP$ is transposable, so $(P^{-1}AP)^T=P^TA^T(P^{-1})^T = D$. Thus, we can see that this relation stays the same, and that the matrix $(P^{-1})^T$ diagonalizes $A^T$, meaning that it is diagonalizable.

**b) Prove that if A is invertible, then $A^{-1}$ is ALSO diagonalizable**

If A is invertible, then that means that $P^{-1}AP = D$ is entirely invertible, because the inverse of P is explicit here, and D is the matrix product of A and P, and P's inverse, so it is obviously invertible by transitive property. So, $(P^{-1}AP)^{-1} = (P)^{-1}(A)^{-1}(P^{-1})^{-1} = P^{-1}A^{-1}P = D^{-1}$. Thus, by the same logic as the previous proof, $A^{-1}$ is also diagonalizable, and the inverse of D is then $D^{-1}= diag(a_1^{-1}, a_2^{-1}, ..., a_n^{-1})$

**c) Prove there exists a square matrix B of the same order of A, such that $B^3 = A$.**

Since $P^{-1}AP=D$, then we can find the matrix A by multiplying LEFT by $P$ and RIGHT by $P^{-1}$, such that $(P(P^{-1}AP)P^{-1}) = (P)D(P^{-1})$, which will reduce $PP^{-1} = I_n$. This means that $A = PDP^{-1}$. So, since $A^k = PD^kP^{-1}$, we know that we can find the *cube root* of A, since $\sqrt[3]{A} = A^{1/3}$. This is because $\sqrt[3]{B^3} = \sqrt[3]{A} \rarr B = \sqrt[3]{A}$.

So, since $A^k = PD^kP^{-1}$, then $A^{1/3}=PD^{1/3}P^{-1}$. And since D is a diagonal matrix, $D^k = diag(a_1^k, a_2^k, ...)$, which means that $D^{1/3}=diag(a_1^{1/3}, ...)$ 

</aside>

# Review Problems

<aside>
‚≠ê Let A and B be $n \times n$ that satisfy: $A^2 + AB - 2B = 0$. Suppose that 2 is NOT an eigenvalue of A, prove that A is singular, if and ONLY if B is singular.

Singular/non-invertible means that det(A) = 0. The other assumption that 2 is not an eigenvalue of A, means that 2 is not a root in the *characteristic polynomial* of A: $C_A(\lambda) = det(\lambda I - A)$.

So, since we have that the characteristic polynomial, we can replace the eigenvalue variable in it to become $C_A(2) = det(2I - A) \ne 0$. This is because 2 is NOT an eigenvalue, so it is not a root, meaning that it does NOT equate to zero. If it were an eigenvalue, then it would equate to zero. So, we have that $A^2 + AB - 2B = 0$, so we can rearrange and factor this:

$$
A^2 = 2B - AB\\
A^2 = (2I - A)B
$$

So, by the product rule of determinants we know that $det(PQ) = det(P)det(Q)$, or $det(P^2) = det(P)det(P)$. We can perform a perform a determinant on both sides of this equation:

$$
det(A)^2 = det((2I-A)B)\\
det(A^2) = det(2I-A)det(B)
$$

Thus, we've shown that $det(2I-A)$  is not 0, because it is not an eigenvalue of A. This leads us to finalize the fact that $det(A)$ is 0, when det(B) is 0, because $det(2I-A)$ cannot be zero.

</aside>