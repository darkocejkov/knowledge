# RECAP!

# Header

## Header 2

# Matrices & Transformations

<aside>
ðŸ’¡ This course did not introduce the concept of treating matrices as transformations, but it is imperative to understand because all matrices can be treated as transformations, and enables the intuitive understanding of all other concepts.

</aside>

An important idea in linear algebra is *transformations*. In which we start with one set of objects, and apply some transformation, and set up with a new set of vectors. There are many different kinds of transformations, but here are some very simple things to understand:

â†’ Not all transformations are *linear* transformations. The class of linear tranformations must have 2 things: the distance between all points must stay the same across all points, and that the origin must stay fixed. Think about the transformation of adding the vector $[1,1]$ to all vectors in the space. This is not linear, because it shifts the origin of the space away from (0,0).

â†’ In this course, we only deal with linear transformations

# Linear Independence (Linearity)

Linearity is a way to describe the direction of vectors. We compare two vectors $[x,y]$  and $[a,b]$ to see if their entries are *proportional* between vectors. Think of a vector as a direction, because we can scale any vector by a $\R$ number and represent the *line* formed by that vector's direction. If we are given two vectors, the only way to see if two vectors are linearly independent (by calculation) is to see whether we can write one as a scalar multiple of the other: $A = rX$. If you can, then this is a linear combination, and they are *linearly dependent*. 

# Determinants

## Determinant Properties

<aside>
ðŸ’¡ Determinants of diagonal, lower/upper triangles are the product of all entries on the main diagonal.

> We can use this to our advantage, and as we reduce any matrix into REF and then find the determinant, however, elemtary operations will change the determinant, which is another property of the determinant.
> 
</aside>

<aside>
ðŸ’¡ If a matrix has two *identical* or *proportional* rows, then its determinant is 0.

</aside>

<aside>
ðŸ’¡ If a matrix has ANY entirely-zero rows, then its determinant is 0.

> Because the determinant can be calculated along any row's expansion, and because 0*0*...*0 is always 0.
> 
</aside>

<aside>
ðŸ’¡ Elementary Operations and Determinants

a) Multiplying a row of A by *r*: $det(A_1) = rdet(A)$

b) Swapping two rows of A: $det(A_2) = -det(A)$

c) Adding a multiple of one row to another: $det(A_3) = det(A)$

</aside>

We can use these properties to know how the determinant changes as we reduce it to an easier form, like lower or upper triangular.

One of the theorems outlined in the textbook uses a similar method to easily disqualify matrices from have a non-zero determinant, and that is using the rows/cols of the matrix and creating identical rows using the c) elementary operation

<aside>
ðŸ’¡ $det(A) = det(A^T)$

</aside>

<aside>
ðŸ’¡ $det(AB) = det(A)det(B)$

</aside>

<aside>
ðŸ’¡ $det(cA) = c^ndet(A)$

</aside>

<aside>
ðŸ’¡ $det(A^{-1})=\frac{1}{det(A)}$

</aside>

## Determinants as Scalars

â†’ The determinant is a property of a SQUARE matrix. Matrices that are not square, do not have determinants, because we cannot calculate them.

â†’ The determinant is a representation of how much a matrix transformation will scale the area of the basis. It is a scalar. All vectors and transversely, all shapes and planes will be scaled by the determinant of the transformed basis vectors.

For example, if the basis vectors are transformed by a *shearing* transformation, without any scaling, then the area that is formed by the sum $e_1+e_2$ is not scaled, but it turns that *square* into a *parallelogram*. The area of a parallelogram is identical to the square, because the height and base are the same.

*Thus, the determinant of a matrix represents the scaling of the transformation that will be applied.*

<aside>
ðŸ’¡ This sounds quite weird, because area is not negative, there is no negative area. However, determinants can be negative even though it scales area. This is because the sign of the determinant represents the orientation of the subspace. So, if a determinant is negative, that means that it will change the orientation of the basis vectors. If you think about it, the *y* basis (vertical) and the *x basis* (horizontal) will flip orientation so that *x* can be on the *left* of *y*, when it is originally on the left.

TL;DR - it represents a flip/mirror over the axis of the transformation.

</aside>

Determinants are then dependent on the values of the entries, because the entries determine how the basis vectors are transformed, and the value of the entry is the scale of that transformation.

## Determinants and Linearity

If we refer back to what it means to be linearly independent, we are comparing the direction of two or more vectors (a set). Determinants are a way to test whether or not a matrix formed by more than 1 vector is linearly independent or not. Meaning that we are trying to see if at least 2 vectors are related (proportional).

## Determinants by Co-factor Expansion

Determinants are calculated through what is called the **co-factor expansion**, in which we choose a row to calculate the determinant on, and use that row/col (i,j) entry with the submatrix formed by excluding each entry's row and column.

This is a **recursive** algorithm, meaning that there are matrices in which we have to *repeat* the previous steps to a submatrix. 

In order to simplify this process, lets just assume that for any 1x1 matrix $[a]$ for example, it's determinant is $a$, the only entry. easy, done. However, for any SQUARE matrix of order $n \ge 2$, we need to do more. 

This sounds really confusing, but is quite simple.

## Determinants, Co-factors, Kronecker's Delta, and Cramer's Rule

This part of the textbook is probably the worst one personally, it's so unintuitive and fucking stupidly done. Fortunately, it is made a lot simple by understanding that we can use a property of the **Cramer's Rule.**

<aside>
ðŸ’¡ Cramer's Rule allows us to solve a *system of linear equations* $Ax = b$ when A is square, and that the determinant of A is not 0. This means that A is invertible, and there's a *unique solution*, but we DO NOT need to compute the inverse of A, because we can compute the entries of *x* given the determinants of A.

This rule is really weird, because it allows us to replace a *column* or *row* with the vector *b.* If you think about it, it makes a lot of sense, since $x = \frac{1}{det(A)}adj(A)b$. here, we're calculating x by $\frac{adj(A)b}{det(A)}$, meaning that the top adjoint-matrix is multiplied by the column vector *b*, which is essentially replacing a row or column. 

</aside>

We can use the nifty-ness of the cramer's rule that to solve a linear system, we replace single columns or rows with another vector. This is seen when we try to involve this with *kronecker's delta* rule, that $a_{i1}C_{j1}+a_{i2}C_{j2}+...+a_{in}C_{in}=\delta_{ij} \det(A)$, where $\delta_{ij}$ is 1 if i = j, and 0 otherwise. Essentially, this allows us to compute different variations of a matrix's determinant for different coefficients $a$ of this cofactor expansion, as long all $C$ are in the same row OR column.

Here are the steps:

![Untitled](RECAP!%20d5e4108e0e3945f48152bd60aef71215/Untitled.png)

<aside>
ðŸ’¡ Given this matrix A, we can calculate this cofactor expansion WITHOUT calculating each cofactor. However, it is simplest to use the rules of elementary row operations and cramer's rule this way:

![Untitled](RECAP!%20d5e4108e0e3945f48152bd60aef71215/Untitled%201.png)

</aside>

This is because we are computing the cofactor expansion along the 1st column, but not really using the coefficients that emerge from the original matrix A, so in order to compute the cofactor expansion, we need to "import" those coefficients, which is allowed through the Cramer's Rule.

## Determinants and Invertible Matrices

The *determinant* as a scalar helps us to determine if the matrix is 1) square and 2) linearly independent, the two things required for invertibility.

## Determinants and Cross-product

This section is very simple, as it is just restating the cross product of two $\R^3$ vectors in terms of the determinant. We cannot compute determinants of non-square matrices, so we have to work around the fact that we are dealing with TWO vectors, each having THREE unknowns. The way to do this is to include the basis vectors $i, j,k$ as stand-ins in their own vector. Despite the fact that these vectors don't belong in a matrix of value entries, we use them as a notation. All we do is compute the determinant of the 3x3 matrix, on the basis-row/column, multiplying each submatrix by the entry. This is a way of assigning the values of the determinants to the respective entry it has in the cross-product vector (if we remember, the result of a cross-product is another vector that is orthogonal to both entry vectors).

The use here is that determinant properties apply here, and it is very easy to see proportional vectors/rows.

# Cross-Product

is a function that outputs a third vector, given two vectors in $\R^3$. It is important to know that you cannot compute a vector for any other $\R^n$, because it is in the nature of the function.

Importantly, the output is a third vector, that will be *orthogonal* to both vectors that are input into the cross-product. The MAGNITUDE of the cross-product (the length of the vector) will represent the AREA of the parallelogram that is formed by the two input vectors. This GIF to the right illustrates the idea perfectly. 

The cross product of LINEARLY DEPENDENT vectors (same or opposite directions) is 0. The maximum *area* that is achieved is when the vectors are orthogonal. One interesting thing as well is that the *direction* of the cross-product depends on the orientation of the two vectors. This is called the *right-hand rule*, because it is well illustrated with your right hand. With the vector *a* being on your index finger, and *b* on your forefinger, the direction of the cross-product emerges in the thumb. So if *a* is to the right of *b*, then the cross-product is in the negative direction.

![https://upload.wikimedia.org/wikipedia/commons/thumb/6/6e/Cross_product.gif/220px-Cross_product.gif](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6e/Cross_product.gif/220px-Cross_product.gif)

Because the magnitude of the cross-product represents the area of the parallelogram/rectangle formed by the two vectors, then we can use it in many different ways. We know that the area of the triangle formed by vectors is equivalent to half the area of the parallelogram.

# Subspaces of $\R^n$

A subspace is kinda exactly what it sounds like, it's a subspace of vectors that follow a certain criterion or condition. For example, the set of all vectors, whose sum is equal to 10 is a subspace. 

What we are looking for are *linear* subspaces, meaning they follow the rules of linearity. Here are the three rules:

<aside>
ðŸ’¡ 1) Zero vector is ALWAYS in the subspace

2) subspace is CLOSED under vector addition

3) subspace is CLOSED under vector scalar multiplication

</aside>

â†’ "closed" means that the result of these operations, done with vectors from the subspace, will also result in something in that same subspace.

for example, the set of even numbers wouldn't be a linear subspace, because:

1) 0 vector is not included (0 is neither even nor odd)

2) even + even = even, so it is closed

3) even * odd = even, even * even = even, so it is closed.

A subspace is formed by notating what the format of a vector is in the subspace, and the criteria, based on the elements of the vector. Subspaces can also be formed from a *linear system*, because if you think about it, a general solution is it's own space of vectors that would satisfy the system simultaneously. In order to see whether a subset of $\R^n$ is a subspace of that $\R^n$, we need to test the three conditions.

a) **PROVE NOT A SUBSPACE**: if there is a case in which one of the cases fail, then it is not a subspace.

AT LEAST 1 condition must fail for it to be disqualified as a subspace.

b) **PROVE IS A SUBSPACE**: must prove generally each of the 3 conditions holds in $\R^n$.

## Subspaces and Sets of vectors

Given a set of vectors, we want to know if these vectors **span** certain spaces. The span of a set is equivalent to all the linear combinations possible from using the set of vectors, so in a way, it's like saying that the *span* of a set of vectors is the *domain* of a function. It is all the possible resulting vectors that we can obtain from creating a linear combination of vectors.

A set of vectors can also be linearly independent, or dependent, depending on the vectors themselves. This means that two or more vectors are proportional to each other, and this effectively, reduces the span of the set given the number of vectors. Because two or more vectors are proportional, or dependent, then they *contribute* the same direction to the span, and thus do not increase the span of the set of vectors.

<aside>
ðŸ’¡ The zero vector is linearly dependent to all vectors, because it is a 0 multiple of any vector, so, it does not increase the span of the set, and makes the set linearly dependent.

</aside>

When we are given a set of vectors, we want to know some basic things about the space that they span:

i) do they span $\R^n$?

ii) if not, what is the *criteria* for vectors in this subspace?

This conversation brings up **rank**, because the rank is the evaluation of the number of *leading 1's* in the RREF of the set of vectors, or the leading numbers in the REF. This is just their computational method, because the concept of the rank is much, much more. If we think back to matrix transformations, all systems or matrices can be a transformation of the space. In essence, a set of vectors or the matrix of that system, will transform the $\R^n$ space to conform to the subspace; and if the span of the system is $\R^n$, then no vectors are lost or **nulled** (reduced to $\overrightarrow{0})$.

Rank is essentially the *output dimension* of the transformed vectors. If we apply a **full rank** ($rank(A)= n$) tranformation to $\R^n$, then the dimension of the output vectors will also be in $\R^n$. If it is less ($rank(A) < n$), then the subspace formed by the matrix A is NOT $\R^n$, and it spans a subspace of $\R^n$, like a plane or line, or even a single point.

So we know if a set of vectors spans any $\R^n$ because it must be full rank.

We can also determine the criteria of vectors for that subspace using the RREF methods of gaussian elimination. This is useful because when we are testing the rank, we need to get to REF/RREF, so to find the criteria for any vectors, we just find the augmented matrix $[A\; |\;b]$ in RREF, where $b$ is an arbitrary vector in $\R^n$.

However, to show that every vector in $\R^n$ is a linear combination of a set of vectors, we need to check for the *rank* of that set to equal the number of rows (ie, $n$). Despite the fact that the number of vectors in the set might be larger than *n*, one of those extra vectors is probably dependent with to the set and we can still represent every vector in $\R^n$ with that dependent vector.

## Solution Sets of Sets of Vectors

We want to find the solution set of the subspace *spanned by a set of vectors* V. The way to do this, is to gather all the vectors of V into a matrix, A. The *homogenous linear system* must be consistent for this solution to work. So, we want to know, for which values of some arbitrary vector $x$, do these sets of vectors have a consistent system.

In the example on the right, the set of V is in an augmented matrix with *x*, and we can row-reduce until we reach a point at which we have rows of zeroes. We can see that the last two rows are the only two zero-rows, and we have $-5x_1+2x_2+x_3 = 0$, and $-x_1+x_4 = 0$. 

The system is ONLY consistent, when these two equations equals 0, otherwise, we have that $0v_1+0v_2+0v_3 = -x_1+x_4$, and if $-x_1+x_4$ is not 0, then this equation is impossible, it becomes undefined.

![Untitled](RECAP!%20d5e4108e0e3945f48152bd60aef71215/Untitled%202.png)

So, we take these two equations and form a *homogenous linear system* with them. We know that the solution set to this new, secondary linear system will be the solution set to the subspace given by the vectors from V. Because they define when the combinations of V are undefined or not, so the other two results $x_1$ and $2x_1-x_2$ becomes irrelevant to the solution set, because they are always defined.

# Similar Matrices

Given two matrices, A and B, if they are similar, then $P^{-1}AP = B$ means that they are related by an invertible matrix P. Based on the 3B1B video on *change of basis*, these similar matrices are what it means to *change basis* through translation. The matrix P is a translation matrix, and A is a transformation matrix. In this way, if we want to apply a transformation to a different basis from B, say to the basis of A, then we need to apply the transformation matrix A to P, which is a vector that translates coordinates between A and B bases. Then, we need to untranslate.